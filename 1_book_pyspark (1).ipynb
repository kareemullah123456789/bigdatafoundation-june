{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqKNMjF9IQ_E",
        "outputId": "8a05574b-0192-40df-daa3-fe728717dd7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Check if a SparkSession already exists\n",
        "spark = SparkSession.getActiveSession()\n",
        "\n",
        "# If exists, stop it\n",
        "if spark is not None:\n",
        "    spark.stop()\n",
        "    print(\"Stopped existing SparkSession\")\n",
        "\n",
        "# Create a new SparkSession\n",
        "spark = (SparkSession\n",
        "         .builder\n",
        "         .appName(\"Analyzing the vocabulary of Pride and Prejudice.\")\n",
        "         .getOrCreate())"
      ],
      "metadata": {
        "id": "uTUlKcGUIn_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr7fFmuyIEHi"
      },
      "outputs": [],
      "source": [
        "spark.sparkContext.setLogLevel(\"ALL\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " spark.read"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXQ5sc16Igvn",
        "outputId": "5b1c1dbe-67e4-49b4-ad9f-e27c2ea3635a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.readwriter.DataFrameReader at 0x7bdbf69d7e10>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(spark.read)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysJnR37qN0g3",
        "outputId": "058cc590-3af8-402a-a2e4-5d9941d33b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_df',\n",
              " '_jreader',\n",
              " '_set_opts',\n",
              " '_spark',\n",
              " 'csv',\n",
              " 'format',\n",
              " 'jdbc',\n",
              " 'json',\n",
              " 'load',\n",
              " 'option',\n",
              " 'options',\n",
              " 'orc',\n",
              " 'parquet',\n",
              " 'schema',\n",
              " 'table',\n",
              " 'text']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "\n",
        "# spark = SparkSession.builder.appName(\n",
        "#     \"Counting word occurences from a book.\"\n",
        "# ).getOrCreate()\n",
        "\n",
        "# spark.sparkContext.setLogLevel(\"WARN\")"
      ],
      "metadata": {
        "id": "_WY74oUsOG5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# F.col()\n",
        "# F.lower()\n",
        "# F.count()\n",
        "# F.split()"
      ],
      "metadata": {
        "id": "lOvr36i1kBfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import requests\n",
        "# import os\n",
        "# from pyspark.sql import SparkSession\n",
        "# from pyspark.sql import functions as F\n",
        "\n",
        "# # GitHub raw file URL (replace with your own file URL)\n",
        "# github_url = \"https://raw.githubusercontent.com/kareemullah123456789/DataAnalysisWithPythonAndPySpark-Data/trunk/gutenberg_books/1342-0.txt\"\n",
        "\n",
        "# # Local filename to save the downloaded file\n",
        "# local_filename = \"1342-0.txt\"\n",
        "\n",
        "# # Download the file from GitHub\n",
        "# response = requests.get(github_url)\n",
        "# with open(local_filename, \"wb\") as f:\n",
        "#     f.write(response.content)\n",
        "\n",
        "# # Initialize Spark session\n",
        "# spark = SparkSession.builder.appName(\"GitHubTextAnalysis\").getOrCreate()\n",
        "\n",
        "# # Load the downloaded text file into PySpark\n",
        "# text_df = spark.read.text(local_filename)\n",
        "\n",
        "# # Process the text: Tokenization, Cleaning, and Word Count\n",
        "# results = (\n",
        "#     text_df\n",
        "#     .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))  # Split into words\n",
        "#     .select(F.explode(F.col(\"line\")).alias(\"word\"))  # Flatten into rows\n",
        "#     .select(F.lower(F.col(\"word\")).alias(\"word\"))  # Convert to lowercase\n",
        "#     .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))  # Extract words only\n",
        "#     .where(F.col(\"word\") != \"\")  # Remove empty words\n",
        "#     .groupBy(\"word\")\n",
        "#     .count()\n",
        "# )\n",
        "\n",
        "# # Show top 10 most frequent words\n",
        "# results.orderBy(\"count\", ascending=False).show(10)\n",
        "\n",
        "# # Save results as a CSV file\n",
        "# results.coalesce(1).write.csv(\"./github_results.csv\", header=True)\n"
      ],
      "metadata": {
        "id": "39xiz3KjN4Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxR_28rVQVXk",
        "outputId": "54845254-d971-4ec4-b418-3441ef8ea9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls /content/drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfIwH_ygkySu",
        "outputId": "967a58cf-99d0-4200-c0c2-62153012a1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mMyDrive\u001b[0m/  \u001b[01;34mOthercomputers\u001b[0m/  \u001b[01;34mShareddrives\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ls /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "v8QsrJB6k5LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/cde_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jftX71ZQexc",
        "outputId": "abfb95a6-9baf-4d37-d114-bb4854b46292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " data.csv\n",
            "'pyngrok_UI_CODE_Working_with_RDDs_in_PySpark_(2).ipynb'\n",
            " pyspark_tutorial.ipynb\n",
            " sales_data.csv\n",
            " sample_data.csv\n",
            " Section_2_Resilient_Distributed_Datasets_Transformations.ipynb\n",
            " Section_3_Resilient_Distributed_Datasets_Actions.ipynb\n",
            " Section_4_Spark_DataFrames_and_Transformations.ipynb\n",
            " Spark_SQL.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Mount Google Drive (ensure it's mounted before running)\n",
        "drive_path = \"/content/drive/MyDrive/cde_data\"  # Change this to your preferred folder in Google Drive\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# GitHub raw file URL\n",
        "github_url = \"https://raw.githubusercontent.com/kareemullah123456789/DataAnalysisWithPythonAndPySpark-Data/trunk/gutenberg_books/1342-0.txt\"\n",
        "\n",
        "# Save file in Google Drive\n",
        "local_filename = os.path.join(drive_path, \"1342-0.txt\")\n",
        "\n",
        "# Download the file from GitHub\n",
        "response = requests.get(github_url)\n",
        "with open(local_filename, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(f\"File downloaded to: {local_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKoVv1NuQiBl",
        "outputId": "7f4db1ea-7e3a-43fc-89ac-faa3771556d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded to: /content/drive/MyDrive/cde_data/1342-0.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"prideandprejudice\").getOrCreate()\n",
        "\n",
        "# Load the text file from Google Drive\n",
        "text_df = spark.read.text(local_filename)\n",
        "\n",
        "# Process the text: Tokenization, Cleaning, and Word Count\n",
        "results = (\n",
        "    text_df\n",
        "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))  # Split into words\n",
        "    .select(F.explode(F.col(\"line\")).alias(\"word\"))  # Flatten into rows\n",
        "    .select(F.lower(F.col(\"word\")).alias(\"word\"))  # Convert to lowercase\n",
        "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))  # Extract words only\n",
        "    .where(F.col(\"word\") != \"\")  # Remove empty words\n",
        "    .groupBy(\"word\")\n",
        "    .count()\n",
        ")\n",
        "\n",
        "# Show top 10 most frequent words\n",
        "results.orderBy(\"count\", ascending=False).show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R_hJ-jMQwMK",
        "outputId": "7f1094c9-f689-4e0a-a2a6-3a358d7a19ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "| the| 4480|\n",
            "|  to| 4218|\n",
            "|  of| 3711|\n",
            "| and| 3504|\n",
            "| her| 2199|\n",
            "|   a| 1982|\n",
            "|  in| 1909|\n",
            "| was| 1838|\n",
            "|   i| 1749|\n",
            "| she| 1668|\n",
            "+----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = os.path.join(drive_path, \"prideandprejudice.csv\")\n",
        "results.coalesce(1).write.csv(output_path, header=True)\n",
        "\n",
        "print(f\"Results saved to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "1JeWgdzKQ91j",
        "outputId": "25bfbab3-8fd3-4adb-b563-54f82e16a503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_ALREADY_EXISTS] Path file:/content/drive/MyDrive/cde_data/prideandprejudice.csv already exists. Set mode as \"overwrite\" to overwrite the existing path.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-7c02642977e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prideandprejudice.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Results saved to: {output_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m         )\n\u001b[0;32m-> 1864\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m     def orc(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/content/drive/MyDrive/cde_data/prideandprejudice.csv already exists. Set mode as \"overwrite\" to overwrite the existing path."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " book = spark.read.text(local_filename)\n",
        " book\n",
        " # DataFrame[value: string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Frk2ppfRowG",
        "outputId": "f528c24e-f4b1-4c31-8a10-21cecd16773a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[value: string]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(book)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOZjmVbVm0Jp",
        "outputId": "19418ca5-34af-4558-9d57-ded5642ca02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on DataFrame in module pyspark.sql.dataframe object:\n",
            "\n",
            "class DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
            " |  DataFrame(jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
            " |  \n",
            " |  A distributed collection of data grouped into named columns.\n",
            " |  \n",
            " |  .. versionadded:: 1.3.0\n",
            " |  \n",
            " |  .. versionchanged:: 3.4.0\n",
            " |      Supports Spark Connect.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
            " |  and can be created using various functions in :class:`SparkSession`:\n",
            " |  \n",
            " |  >>> people = spark.createDataFrame([\n",
            " |  ...     {\"deptId\": 1, \"age\": 40, \"name\": \"Hyukjin Kwon\", \"gender\": \"M\", \"salary\": 50},\n",
            " |  ...     {\"deptId\": 1, \"age\": 50, \"name\": \"Takuya Ueshin\", \"gender\": \"M\", \"salary\": 100},\n",
            " |  ...     {\"deptId\": 2, \"age\": 60, \"name\": \"Xinrong Meng\", \"gender\": \"F\", \"salary\": 150},\n",
            " |  ...     {\"deptId\": 3, \"age\": 20, \"name\": \"Haejoon Lee\", \"gender\": \"M\", \"salary\": 200}\n",
            " |  ... ])\n",
            " |  \n",
            " |  Once created, it can be manipulated using the various domain-specific-language\n",
            " |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
            " |  \n",
            " |  To select a column from the :class:`DataFrame`, use the apply method:\n",
            " |  \n",
            " |  >>> age_col = people.age\n",
            " |  \n",
            " |  A more concrete example:\n",
            " |  \n",
            " |  >>> # To create DataFrame using SparkSession\n",
            " |  ... department = spark.createDataFrame([\n",
            " |  ...     {\"id\": 1, \"name\": \"PySpark\"},\n",
            " |  ...     {\"id\": 2, \"name\": \"ML\"},\n",
            " |  ...     {\"id\": 3, \"name\": \"Spark SQL\"}\n",
            " |  ... ])\n",
            " |  \n",
            " |  >>> people.filter(people.age > 30).join(\n",
            " |  ...     department, people.deptId == department.id).groupBy(\n",
            " |  ...     department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"}).show()\n",
            " |  +-------+------+-----------+--------+\n",
            " |  |   name|gender|avg(salary)|max(age)|\n",
            " |  +-------+------+-----------+--------+\n",
            " |  |     ML|     F|      150.0|      60|\n",
            " |  |PySpark|     M|       75.0|      50|\n",
            " |  +-------+------+-----------+--------+\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  A DataFrame should only be created as described above. It should not be directly\n",
            " |  created via using the constructor.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DataFrame\n",
            " |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n",
            " |      pyspark.sql.pandas.conversion.PandasConversionMixin\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __dir__(self) -> List[str]\n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.functions import lit\n",
            " |      \n",
            " |      Create a dataframe with a column named 'id'.\n",
            " |      \n",
            " |      >>> df = spark.range(3)\n",
            " |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Includes column id\n",
            " |      ['id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal', 'isStreaming']\n",
            " |      \n",
            " |      Add a column named 'i_like_pancakes'.\n",
            " |      \n",
            " |      >>> df = df.withColumn('i_like_pancakes', lit(1))\n",
            " |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Includes columns i_like_pancakes, id\n",
            " |      ['i_like_pancakes', 'id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal']\n",
            " |      \n",
            " |      Try to add an existed column 'inputFiles'.\n",
            " |      \n",
            " |      >>> df = df.withColumn('inputFiles', lit(2))\n",
            " |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Doesn't duplicate inputFiles\n",
            " |      ['i_like_pancakes', 'id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal']\n",
            " |      \n",
            " |      Try to add a column named 'id2'.\n",
            " |      \n",
            " |      >>> df = df.withColumn('id2', lit(3))\n",
            " |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # result includes id2 and sorted\n",
            " |      ['i_like_pancakes', 'id', 'id2', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty']\n",
            " |      \n",
            " |      Don't include columns that are not valid python identifiers.\n",
            " |      \n",
            " |      >>> df = df.withColumn('1', lit(4))\n",
            " |      >>> df = df.withColumn('name 1', lit(5))\n",
            " |      >>> [attr for attr in dir(df) if attr[0] == 'i'][:7] # Doesn't include 1 or name 1\n",
            " |      ['i_like_pancakes', 'id', 'id2', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty']\n",
            " |  \n",
            " |  __getattr__(self, name: str) -> pyspark.sql.column.Column\n",
            " |      Returns the :class:`Column` denoted by ``name``.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          Column name to return as :class:`Column`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`Column`\n",
            " |          Requested column.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      \n",
            " |      Retrieve a column instance.\n",
            " |      \n",
            " |      >>> df.select(df.age).show()\n",
            " |      +---+\n",
            " |      |age|\n",
            " |      +---+\n",
            " |      |  2|\n",
            " |      |  5|\n",
            " |      +---+\n",
            " |  \n",
            " |  __getitem__(self, item: Union[int, str, pyspark.sql.column.Column, List, Tuple]) -> Union[pyspark.sql.column.Column, ForwardRef('DataFrame')]\n",
            " |      Returns the column as a :class:`Column`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      item : int, str, :class:`Column`, list or tuple\n",
            " |          column index, column name, column, or a list or tuple of columns\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`Column` or :class:`DataFrame`\n",
            " |          a specified column, or a filtered or projected dataframe.\n",
            " |      \n",
            " |          * If the input `item` is an int or str, the output is a :class:`Column`.\n",
            " |      \n",
            " |          * If the input `item` is a :class:`Column`, the output is a :class:`DataFrame`\n",
            " |              filtered by this given :class:`Column`.\n",
            " |      \n",
            " |          * If the input `item` is a list or tuple, the output is a :class:`DataFrame`\n",
            " |              projected by this given list or tuple.\n",
            " |      \n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      \n",
            " |      Retrieve a column instance.\n",
            " |      \n",
            " |      >>> df.select(df['age']).show()\n",
            " |      +---+\n",
            " |      |age|\n",
            " |      +---+\n",
            " |      |  2|\n",
            " |      |  5|\n",
            " |      +---+\n",
            " |      \n",
            " |      >>> df.select(df[1]).show()\n",
            " |      +-----+\n",
            " |      | name|\n",
            " |      +-----+\n",
            " |      |Alice|\n",
            " |      |  Bob|\n",
            " |      +-----+\n",
            " |      \n",
            " |      Select multiple string columns as index.\n",
            " |      \n",
            " |      >>> df[[\"name\", \"age\"]].show()\n",
            " |      +-----+---+\n",
            " |      | name|age|\n",
            " |      +-----+---+\n",
            " |      |Alice|  2|\n",
            " |      |  Bob|  5|\n",
            " |      +-----+---+\n",
            " |      >>> df[df.age > 3].show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |  5| Bob|\n",
            " |      +---+----+\n",
            " |      >>> df[df[0] > 3].show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |  5| Bob|\n",
            " |      +---+----+\n",
            " |  \n",
            " |  __init__(self, jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  agg(self, *exprs: Union[pyspark.sql.column.Column, Dict[str, str]]) -> 'DataFrame'\n",
            " |      Aggregate on the entire :class:`DataFrame` without groups\n",
            " |      (shorthand for ``df.groupBy().agg()``).\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      exprs : :class:`Column` or dict of key and value strings\n",
            " |          Columns or expressions to aggregate DataFrame by.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Aggregated DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql import functions as sf\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.agg({\"age\": \"max\"}).show()\n",
            " |      +--------+\n",
            " |      |max(age)|\n",
            " |      +--------+\n",
            " |      |       5|\n",
            " |      +--------+\n",
            " |      >>> df.agg(sf.min(df.age)).show()\n",
            " |      +--------+\n",
            " |      |min(age)|\n",
            " |      +--------+\n",
            " |      |       2|\n",
            " |      +--------+\n",
            " |  \n",
            " |  alias(self, alias: str) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` with an alias set.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      alias : str\n",
            " |          an alias name to be set for the :class:`DataFrame`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Aliased DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.functions import col, desc\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df_as1 = df.alias(\"df_as1\")\n",
            " |      >>> df_as2 = df.alias(\"df_as2\")\n",
            " |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
            " |      >>> joined_df.select(\n",
            " |      ...     \"df_as1.name\", \"df_as2.name\", \"df_as2.age\").sort(desc(\"df_as1.name\")).show()\n",
            " |      +-----+-----+---+\n",
            " |      | name| name|age|\n",
            " |      +-----+-----+---+\n",
            " |      |  Tom|  Tom| 14|\n",
            " |      |  Bob|  Bob| 16|\n",
            " |      |Alice|Alice| 23|\n",
            " |      +-----+-----+---+\n",
            " |  \n",
            " |  approxQuantile(self, col: Union[str, List[str], Tuple[str]], probabilities: Union[List[float], Tuple[float]], relativeError: float) -> Union[List[float], List[List[float]]]\n",
            " |      Calculates the approximate quantiles of numerical columns of a\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      The result of this algorithm has the following deterministic bound:\n",
            " |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
            " |      probability `p` up to error `err`, then the algorithm will return\n",
            " |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
            " |      close to (p * N). More precisely,\n",
            " |      \n",
            " |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
            " |      \n",
            " |      This method implements a variation of the Greenwald-Khanna\n",
            " |      algorithm (with some speed optimizations). The algorithm was first\n",
            " |      present in [[https://doi.org/10.1145/375663.375670\n",
            " |      Space-efficient Online Computation of Quantile Summaries]]\n",
            " |      by Greenwald and Khanna.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      col: str, tuple or list\n",
            " |          Can be a single column name, or a list of names for multiple columns.\n",
            " |      \n",
            " |          .. versionchanged:: 2.2.0\n",
            " |             Added support for multiple columns.\n",
            " |      probabilities : list or tuple\n",
            " |          a list of quantile probabilities\n",
            " |          Each number must belong to [0, 1].\n",
            " |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
            " |      relativeError : float\n",
            " |          The relative target precision to achieve\n",
            " |          (>= 0). If set to zero, the exact quantiles are computed, which\n",
            " |          could be very expensive. Note that values greater than 1 are\n",
            " |          accepted but gives the same result as 1.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          the approximate quantiles at the given probabilities.\n",
            " |      \n",
            " |          * If the input `col` is a string, the output is a list of floats.\n",
            " |      \n",
            " |          * If the input `col` is a list or tuple of strings, the output is also a\n",
            " |              list, but each element in it is a list of floats, i.e., the output\n",
            " |              is a list of list of floats.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Null values will be ignored in numerical columns before calculation.\n",
            " |      For columns only containing null values, an empty list is returned.\n",
            " |  \n",
            " |  cache(self) -> 'DataFrame'\n",
            " |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK_DESER`).\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Cached DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(1)\n",
            " |      >>> df.cache()\n",
            " |      DataFrame[id: bigint]\n",
            " |      \n",
            " |      >>> df.explain()\n",
            " |      == Physical Plan ==\n",
            " |      AdaptiveSparkPlan isFinalPlan=false\n",
            " |      +- InMemoryTableScan ...\n",
            " |  \n",
            " |  checkpoint(self, eager: bool = True) -> 'DataFrame'\n",
            " |      Returns a checkpointed version of this :class:`DataFrame`. Checkpointing can be used to\n",
            " |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
            " |      iterative algorithms where the plan may grow exponentially. It will be saved to files\n",
            " |      inside the checkpoint directory set with :meth:`SparkContext.setCheckpointDir`.\n",
            " |      \n",
            " |      .. versionadded:: 2.1.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      eager : bool, optional, default True\n",
            " |          Whether to checkpoint this :class:`DataFrame` immediately.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Checkpointed DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is experimental.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import tempfile\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     spark.sparkContext.setCheckpointDir(\"/tmp/bb\")\n",
            " |      ...     df.checkpoint(False)\n",
            " |      DataFrame[age: bigint, name: string]\n",
            " |  \n",
            " |  coalesce(self, numPartitions: int) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
            " |      \n",
            " |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
            " |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
            " |      there will not be a shuffle, instead each of the 100 new partitions will\n",
            " |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
            " |      it will stay at the current number of partitions.\n",
            " |      \n",
            " |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
            " |      this may result in your computation taking place on fewer nodes than\n",
            " |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
            " |      you can call repartition(). This will add a shuffle step, but means the\n",
            " |      current upstream partitions will be executed in parallel (per whatever\n",
            " |      the current partitioning is).\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      numPartitions : int\n",
            " |          specify the target number of partitions\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(10)\n",
            " |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
            " |      1\n",
            " |  \n",
            " |  colRegex(self, colName: str) -> pyspark.sql.column.Column\n",
            " |      Selects column based on the column name specified as a regex and returns it\n",
            " |      as :class:`Column`.\n",
            " |      \n",
            " |      .. versionadded:: 2.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      colName : str\n",
            " |          string, column name specified as a regex.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`Column`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
            " |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
            " |      +----+\n",
            " |      |Col2|\n",
            " |      +----+\n",
            " |      |   1|\n",
            " |      |   2|\n",
            " |      |   3|\n",
            " |      +----+\n",
            " |  \n",
            " |  collect(self) -> List[pyspark.sql.types.Row]\n",
            " |      Returns all the records as a list of :class:`Row`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          List of rows.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.collect()\n",
            " |      [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
            " |  \n",
            " |  corr(self, col1: str, col2: str, method: Optional[str] = None) -> float\n",
            " |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
            " |      Currently only supports the Pearson Correlation Coefficient.\n",
            " |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      col1 : str\n",
            " |          The name of the first column\n",
            " |      col2 : str\n",
            " |          The name of the second column\n",
            " |      method : str, optional\n",
            " |          The correlation method. Currently only supports \"pearson\"\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      float\n",
            " |          Pearson Correlation Coefficient of two columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
            " |      >>> df.corr(\"c1\", \"c2\")\n",
            " |      -0.3592106040535498\n",
            " |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
            " |      >>> df.corr(\"small\", \"bigger\")\n",
            " |      1.0\n",
            " |  \n",
            " |  count(self) -> int\n",
            " |      Returns the number of rows in this :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      int\n",
            " |          Number of rows.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Return the number of rows in the :class:`DataFrame`.\n",
            " |      \n",
            " |      >>> df.count()\n",
            " |      3\n",
            " |  \n",
            " |  cov(self, col1: str, col2: str) -> float\n",
            " |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
            " |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      col1 : str\n",
            " |          The name of the first column\n",
            " |      col2 : str\n",
            " |          The name of the second column\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      float\n",
            " |          Covariance of two columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
            " |      >>> df.cov(\"c1\", \"c2\")\n",
            " |      -18.0\n",
            " |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
            " |      >>> df.cov(\"small\", \"bigger\")\n",
            " |      1.0\n",
            " |  \n",
            " |  createGlobalTempView(self, name: str) -> None\n",
            " |      Creates a global temporary view with this :class:`DataFrame`.\n",
            " |      \n",
            " |      The lifetime of this temporary view is tied to this Spark application.\n",
            " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
            " |      catalog.\n",
            " |      \n",
            " |      .. versionadded:: 2.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          Name of the view.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Create a global temporary view.\n",
            " |      \n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.createGlobalTempView(\"people\")\n",
            " |      >>> df2 = spark.sql(\"SELECT * FROM global_temp.people\")\n",
            " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
            " |      True\n",
            " |      \n",
            " |      Throws an exception if the global temporary view already exists.\n",
            " |      \n",
            " |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
            " |      Traceback (most recent call last):\n",
            " |      ...\n",
            " |      AnalysisException: \"Temporary table 'people' already exists;\"\n",
            " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
            " |      True\n",
            " |  \n",
            " |  createOrReplaceGlobalTempView(self, name: str) -> None\n",
            " |      Creates or replaces a global temporary view using the given name.\n",
            " |      \n",
            " |      The lifetime of this temporary view is tied to this Spark application.\n",
            " |      \n",
            " |      .. versionadded:: 2.2.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          Name of the view.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Create a global temporary view.\n",
            " |      \n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
            " |      \n",
            " |      Replace the global temporary view.\n",
            " |      \n",
            " |      >>> df2 = df.filter(df.age > 3)\n",
            " |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
            " |      >>> df3 = spark.sql(\"SELECT * FROM global_temp.people\")\n",
            " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
            " |      True\n",
            " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
            " |      True\n",
            " |  \n",
            " |  createOrReplaceTempView(self, name: str) -> None\n",
            " |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
            " |      \n",
            " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
            " |      that was used to create this :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          Name of the view.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Create a local temporary view named 'people'.\n",
            " |      \n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.createOrReplaceTempView(\"people\")\n",
            " |      \n",
            " |      Replace the local temporary view.\n",
            " |      \n",
            " |      >>> df2 = df.filter(df.age > 3)\n",
            " |      >>> df2.createOrReplaceTempView(\"people\")\n",
            " |      >>> df3 = spark.sql(\"SELECT * FROM people\")\n",
            " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
            " |      True\n",
            " |      >>> spark.catalog.dropTempView(\"people\")\n",
            " |      True\n",
            " |  \n",
            " |  createTempView(self, name: str) -> None\n",
            " |      Creates a local temporary view with this :class:`DataFrame`.\n",
            " |      \n",
            " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
            " |      that was used to create this :class:`DataFrame`.\n",
            " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
            " |      catalog.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          Name of the view.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Create a local temporary view.\n",
            " |      \n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.createTempView(\"people\")\n",
            " |      >>> df2 = spark.sql(\"SELECT * FROM people\")\n",
            " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
            " |      True\n",
            " |      \n",
            " |      Throw an exception if the table already exists.\n",
            " |      \n",
            " |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
            " |      Traceback (most recent call last):\n",
            " |      ...\n",
            " |      AnalysisException: \"Temporary table 'people' already exists;\"\n",
            " |      >>> spark.catalog.dropTempView(\"people\")\n",
            " |      True\n",
            " |  \n",
            " |  crossJoin(self, other: 'DataFrame') -> 'DataFrame'\n",
            " |      Returns the cartesian product with another :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 2.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Right side of the cartesian product.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Joined DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql import Row\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df2 = spark.createDataFrame(\n",
            " |      ...     [Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
            " |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").show()\n",
            " |      +---+-----+------+\n",
            " |      |age| name|height|\n",
            " |      +---+-----+------+\n",
            " |      | 14|  Tom|    80|\n",
            " |      | 14|  Tom|    85|\n",
            " |      | 23|Alice|    80|\n",
            " |      | 23|Alice|    85|\n",
            " |      | 16|  Bob|    80|\n",
            " |      | 16|  Bob|    85|\n",
            " |      +---+-----+------+\n",
            " |  \n",
            " |  crosstab(self, col1: str, col2: str) -> 'DataFrame'\n",
            " |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
            " |      table.\n",
            " |      The first column of each row will be the distinct values of `col1` and the column names\n",
            " |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
            " |      Pairs that have no occurrences will have zero as their counts.\n",
            " |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      col1 : str\n",
            " |          The name of the first column. Distinct items will make the first item of\n",
            " |          each row.\n",
            " |      col2 : str\n",
            " |          The name of the second column. Distinct items will make the column names\n",
            " |          of the :class:`DataFrame`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Frequency matrix of two columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
            " |      >>> df.crosstab(\"c1\", \"c2\").sort(\"c1_c2\").show()\n",
            " |      +-----+---+---+---+\n",
            " |      |c1_c2| 10| 11|  8|\n",
            " |      +-----+---+---+---+\n",
            " |      |    1|  0|  2|  0|\n",
            " |      |    3|  1|  0|  0|\n",
            " |      |    4|  0|  0|  2|\n",
            " |      +-----+---+---+---+\n",
            " |  \n",
            " |  cube(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
            " |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
            " |      the specified columns, so we can run aggregations on them.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : list, str or :class:`Column`\n",
            " |          columns to create cube by.\n",
            " |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
            " |          or list of them.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`GroupedData`\n",
            " |          Cube of the data by given columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
            " |      +-----+----+-----+\n",
            " |      | name| age|count|\n",
            " |      +-----+----+-----+\n",
            " |      | NULL|NULL|    2|\n",
            " |      | NULL|   2|    1|\n",
            " |      | NULL|   5|    1|\n",
            " |      |Alice|NULL|    1|\n",
            " |      |Alice|   2|    1|\n",
            " |      |  Bob|NULL|    1|\n",
            " |      |  Bob|   5|    1|\n",
            " |      +-----+----+-----+\n",
            " |  \n",
            " |  describe(self, *cols: Union[str, List[str]]) -> 'DataFrame'\n",
            " |      Computes basic statistics for numeric and string columns.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.1\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      This includes count, mean, stddev, min, and max. If no columns are\n",
            " |      given, this function computes statistics for all numerical or string columns.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This function is meant for exploratory data analysis, as we make no\n",
            " |      guarantee about the backward compatibility of the schema of the resulting\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      Use summary for expanded statistics and control over which statistics to compute.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : str, list, optional\n",
            " |           Column name or list of column names to describe by (default All columns).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          A new DataFrame that describes (provides statistics) given DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
            " |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
            " |      ... )\n",
            " |      >>> df.describe(['age']).show()\n",
            " |      +-------+----+\n",
            " |      |summary| age|\n",
            " |      +-------+----+\n",
            " |      |  count|   3|\n",
            " |      |   mean|12.0|\n",
            " |      | stddev| 1.0|\n",
            " |      |    min|  11|\n",
            " |      |    max|  13|\n",
            " |      +-------+----+\n",
            " |      \n",
            " |      >>> df.describe(['age', 'weight', 'height']).show()\n",
            " |      +-------+----+------------------+-----------------+\n",
            " |      |summary| age|            weight|           height|\n",
            " |      +-------+----+------------------+-----------------+\n",
            " |      |  count|   3|                 3|                3|\n",
            " |      |   mean|12.0| 40.73333333333333|            145.0|\n",
            " |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
            " |      |    min|  11|              37.8|            142.2|\n",
            " |      |    max|  13|              44.1|            150.5|\n",
            " |      +-------+----+------------------+-----------------+\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      DataFrame.summary\n",
            " |  \n",
            " |  distinct(self) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with distinct records.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (23, \"Alice\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Return the number of distinct rows in the :class:`DataFrame`\n",
            " |      \n",
            " |      >>> df.distinct().count()\n",
            " |      2\n",
            " |  \n",
            " |  drop(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` without specified columns.\n",
            " |      This is a no-op if the schema doesn't contain the given column name(s).\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols: str or :class:`Column`\n",
            " |          a name of the column, or the :class:`Column` to drop\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame without given columns.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      When an input is a column name, it is treated literally without further interpretation.\n",
            " |      Otherwise, will try to match the equivalent expression.\n",
            " |      So that dropping column by its name `drop(colName)` has different semantic with directly\n",
            " |      dropping the column `drop(col(colName))`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql import Row\n",
            " |      >>> from pyspark.sql.functions import col, lit\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
            " |      \n",
            " |      >>> df.drop('age').show()\n",
            " |      +-----+\n",
            " |      | name|\n",
            " |      +-----+\n",
            " |      |  Tom|\n",
            " |      |Alice|\n",
            " |      |  Bob|\n",
            " |      +-----+\n",
            " |      >>> df.drop(df.age).show()\n",
            " |      +-----+\n",
            " |      | name|\n",
            " |      +-----+\n",
            " |      |  Tom|\n",
            " |      |Alice|\n",
            " |      |  Bob|\n",
            " |      +-----+\n",
            " |      \n",
            " |      Drop the column that joined both DataFrames on.\n",
            " |      \n",
            " |      >>> df.join(df2, df.name == df2.name, 'inner').drop('name').sort('age').show()\n",
            " |      +---+------+\n",
            " |      |age|height|\n",
            " |      +---+------+\n",
            " |      | 14|    80|\n",
            " |      | 16|    85|\n",
            " |      +---+------+\n",
            " |      \n",
            " |      >>> df3 = df.join(df2)\n",
            " |      >>> df3.show()\n",
            " |      +---+-----+------+----+\n",
            " |      |age| name|height|name|\n",
            " |      +---+-----+------+----+\n",
            " |      | 14|  Tom|    80| Tom|\n",
            " |      | 14|  Tom|    85| Bob|\n",
            " |      | 23|Alice|    80| Tom|\n",
            " |      | 23|Alice|    85| Bob|\n",
            " |      | 16|  Bob|    80| Tom|\n",
            " |      | 16|  Bob|    85| Bob|\n",
            " |      +---+-----+------+----+\n",
            " |      \n",
            " |      Drop two column by the same name.\n",
            " |      \n",
            " |      >>> df3.drop(\"name\").show()\n",
            " |      +---+------+\n",
            " |      |age|height|\n",
            " |      +---+------+\n",
            " |      | 14|    80|\n",
            " |      | 14|    85|\n",
            " |      | 23|    80|\n",
            " |      | 23|    85|\n",
            " |      | 16|    80|\n",
            " |      | 16|    85|\n",
            " |      +---+------+\n",
            " |      \n",
            " |      Can not drop col('name') due to ambiguous reference.\n",
            " |      \n",
            " |      >>> df3.drop(col(\"name\")).show()\n",
            " |      Traceback (most recent call last):\n",
            " |      ...\n",
            " |      pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference...\n",
            " |      \n",
            " |      >>> df4 = df.withColumn(\"a.b.c\", lit(1))\n",
            " |      >>> df4.show()\n",
            " |      +---+-----+-----+\n",
            " |      |age| name|a.b.c|\n",
            " |      +---+-----+-----+\n",
            " |      | 14|  Tom|    1|\n",
            " |      | 23|Alice|    1|\n",
            " |      | 16|  Bob|    1|\n",
            " |      +---+-----+-----+\n",
            " |      \n",
            " |      >>> df4.drop(\"a.b.c\").show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      | 14|  Tom|\n",
            " |      | 23|Alice|\n",
            " |      | 16|  Bob|\n",
            " |      +---+-----+\n",
            " |      \n",
            " |      Can not find a column matching the expression \"a.b.c\".\n",
            " |      \n",
            " |      >>> df4.drop(col(\"a.b.c\")).show()\n",
            " |      +---+-----+-----+\n",
            " |      |age| name|a.b.c|\n",
            " |      +---+-----+-----+\n",
            " |      | 14|  Tom|    1|\n",
            " |      | 23|Alice|    1|\n",
            " |      | 16|  Bob|    1|\n",
            " |      +---+-----+-----+\n",
            " |  \n",
            " |  dropDuplicates(self, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
            " |      optionally only considering certain columns.\n",
            " |      \n",
            " |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
            " |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
            " |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
            " |      be and the system will accordingly limit the state. In addition, data older than\n",
            " |      watermark will be dropped to avoid any possibility of duplicates.\n",
            " |      \n",
            " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      subset : List of column names, optional\n",
            " |          List of columns to use for duplicate comparison (default All columns).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame without duplicates.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql import Row\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     Row(name='Alice', age=5, height=80),\n",
            " |      ...     Row(name='Alice', age=5, height=80),\n",
            " |      ...     Row(name='Alice', age=10, height=80)\n",
            " |      ... ])\n",
            " |      \n",
            " |      Deduplicate the same rows.\n",
            " |      \n",
            " |      >>> df.dropDuplicates().show()\n",
            " |      +-----+---+------+\n",
            " |      | name|age|height|\n",
            " |      +-----+---+------+\n",
            " |      |Alice|  5|    80|\n",
            " |      |Alice| 10|    80|\n",
            " |      +-----+---+------+\n",
            " |      \n",
            " |      Deduplicate values on 'name' and 'height' columns.\n",
            " |      \n",
            " |      >>> df.dropDuplicates(['name', 'height']).show()\n",
            " |      +-----+---+------+\n",
            " |      | name|age|height|\n",
            " |      +-----+---+------+\n",
            " |      |Alice|  5|    80|\n",
            " |      +-----+---+------+\n",
            " |  \n",
            " |  dropDuplicatesWithinWatermark(self, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
            " |       optionally only considering certain columns, within watermark.\n",
            " |      \n",
            " |       This only works with streaming :class:`DataFrame`, and watermark for the input\n",
            " |       :class:`DataFrame` must be set via :func:`withWatermark`.\n",
            " |      \n",
            " |      For a streaming :class:`DataFrame`, this will keep all data across triggers as intermediate\n",
            " |      state to drop duplicated rows. The state will be kept to guarantee the semantic, \"Events\n",
            " |      are deduplicated as long as the time distance of earliest and latest events are smaller\n",
            " |      than the delay threshold of watermark.\" Users are encouraged to set the delay threshold of\n",
            " |      watermark longer than max timestamp differences among duplicated events.\n",
            " |      \n",
            " |      Note: too late data older than watermark will be dropped.\n",
            " |      \n",
            " |       .. versionadded:: 3.5.0\n",
            " |      \n",
            " |       Parameters\n",
            " |       ----------\n",
            " |       subset : List of column names, optional\n",
            " |           List of columns to use for duplicate comparison (default All columns).\n",
            " |      \n",
            " |       Returns\n",
            " |       -------\n",
            " |       :class:`DataFrame`\n",
            " |           DataFrame without duplicates.\n",
            " |      \n",
            " |       Notes\n",
            " |       -----\n",
            " |       Supports Spark Connect.\n",
            " |      \n",
            " |       Examples\n",
            " |       --------\n",
            " |       >>> from pyspark.sql import Row\n",
            " |       >>> from pyspark.sql.functions import timestamp_seconds\n",
            " |       >>> df = spark.readStream.format(\"rate\").load().selectExpr(\n",
            " |       ...     \"value % 5 AS value\", \"timestamp\")\n",
            " |       >>> df.select(\"value\", df.timestamp.alias(\"time\")).withWatermark(\"time\", '10 minutes')\n",
            " |       DataFrame[value: bigint, time: timestamp]\n",
            " |      \n",
            " |       Deduplicate the same rows.\n",
            " |      \n",
            " |       >>> df.dropDuplicatesWithinWatermark() # doctest: +SKIP\n",
            " |      \n",
            " |       Deduplicate values on 'value' columns.\n",
            " |      \n",
            " |       >>> df.dropDuplicatesWithinWatermark(['value'])  # doctest: +SKIP\n",
            " |  \n",
            " |  drop_duplicates = dropDuplicates(self, subset=None)\n",
            " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
            " |      \n",
            " |      .. versionadded:: 1.4\n",
            " |  \n",
            " |  dropna(self, how: str = 'any', thresh: Optional[int] = None, subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
            " |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.1\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      how : str, optional\n",
            " |          'any' or 'all'.\n",
            " |          If 'any', drop a row if it contains any nulls.\n",
            " |          If 'all', drop a row only if all its values are null.\n",
            " |      thresh: int, optional\n",
            " |          default None\n",
            " |          If specified, drop rows that have less than `thresh` non-null values.\n",
            " |          This overwrites the `how` parameter.\n",
            " |      subset : str, tuple or list, optional\n",
            " |          optional list of column names to consider.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with null only rows excluded.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql import Row\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
            " |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
            " |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
            " |      ...     Row(age=None, height=None, name=None),\n",
            " |      ... ])\n",
            " |      >>> df.na.drop().show()\n",
            " |      +---+------+-----+\n",
            " |      |age|height| name|\n",
            " |      +---+------+-----+\n",
            " |      | 10|    80|Alice|\n",
            " |      +---+------+-----+\n",
            " |  \n",
            " |  exceptAll(self, other: 'DataFrame') -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
            " |      not in another :class:`DataFrame` while preserving duplicates.\n",
            " |      \n",
            " |      This is equivalent to `EXCEPT ALL` in SQL.\n",
            " |      As standard in SQL, this function resolves columns by position (not by name).\n",
            " |      \n",
            " |      .. versionadded:: 2.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          The other :class:`DataFrame` to compare to.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df1 = spark.createDataFrame(\n",
            " |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
            " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
            " |      >>> df1.exceptAll(df2).show()\n",
            " |      +---+---+\n",
            " |      | C1| C2|\n",
            " |      +---+---+\n",
            " |      |  a|  1|\n",
            " |      |  a|  1|\n",
            " |      |  a|  2|\n",
            " |      |  c|  4|\n",
            " |      +---+---+\n",
            " |  \n",
            " |  explain(self, extended: Union[bool, str, NoneType] = None, mode: Optional[str] = None) -> None\n",
            " |      Prints the (logical and physical) plans to the console for debugging purposes.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      extended : bool, optional\n",
            " |          default ``False``. If ``False``, prints only the physical plan.\n",
            " |          When this is a string without specifying the ``mode``, it works as the mode is\n",
            " |          specified.\n",
            " |      mode : str, optional\n",
            " |          specifies the expected output format of plans.\n",
            " |      \n",
            " |          * ``simple``: Print only a physical plan.\n",
            " |          * ``extended``: Print both logical and physical plans.\n",
            " |          * ``codegen``: Print a physical plan and generated codes if they are available.\n",
            " |          * ``cost``: Print a logical plan and statistics if they are available.\n",
            " |          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n",
            " |      \n",
            " |          .. versionchanged:: 3.0.0\n",
            " |             Added optional argument `mode` to specify the expected output format of plans.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Print out the physical plan only (default).\n",
            " |      \n",
            " |      >>> df.explain()  # doctest: +SKIP\n",
            " |      == Physical Plan ==\n",
            " |      *(1) Scan ExistingRDD[age...,name...]\n",
            " |      \n",
            " |      Print out all of the parsed, analyzed, optimized and physical plans.\n",
            " |      \n",
            " |      >>> df.explain(True)\n",
            " |      == Parsed Logical Plan ==\n",
            " |      ...\n",
            " |      == Analyzed Logical Plan ==\n",
            " |      ...\n",
            " |      == Optimized Logical Plan ==\n",
            " |      ...\n",
            " |      == Physical Plan ==\n",
            " |      ...\n",
            " |      \n",
            " |      Print out the plans with two sections: a physical plan outline and node details\n",
            " |      \n",
            " |      >>> df.explain(mode=\"formatted\")  # doctest: +SKIP\n",
            " |      == Physical Plan ==\n",
            " |      * Scan ExistingRDD (...)\n",
            " |      (1) Scan ExistingRDD [codegen id : ...]\n",
            " |      Output [2]: [age..., name...]\n",
            " |      ...\n",
            " |      \n",
            " |      Print a logical plan and statistics if they are available.\n",
            " |      \n",
            " |      >>> df.explain(\"cost\")\n",
            " |      == Optimized Logical Plan ==\n",
            " |      ...Statistics...\n",
            " |      ...\n",
            " |  \n",
            " |  fillna(self, value: Union[ForwardRef('LiteralType'), Dict[str, ForwardRef('LiteralType')]], subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
            " |      Replace null values, alias for ``na.fill()``.\n",
            " |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.1\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      value : int, float, string, bool or dict\n",
            " |          Value to replace null values with.\n",
            " |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
            " |          from column name (string) to replacement value. The replacement value must be\n",
            " |          an int, float, boolean, or string.\n",
            " |      subset : str, tuple or list, optional\n",
            " |          optional list of column names to consider.\n",
            " |          Columns specified in subset that do not have matching data types are ignored.\n",
            " |          For example, if `value` is a string, and subset contains a non-string column,\n",
            " |          then the non-string column is simply ignored.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with replaced null values.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (10, 80.5, \"Alice\", None),\n",
            " |      ...     (5, None, \"Bob\", None),\n",
            " |      ...     (None, None, \"Tom\", None),\n",
            " |      ...     (None, None, None, True)],\n",
            " |      ...     schema=[\"age\", \"height\", \"name\", \"bool\"])\n",
            " |      \n",
            " |      Fill all null values with 50 for numeric columns.\n",
            " |      \n",
            " |      >>> df.na.fill(50).show()\n",
            " |      +---+------+-----+----+\n",
            " |      |age|height| name|bool|\n",
            " |      +---+------+-----+----+\n",
            " |      | 10|  80.5|Alice|NULL|\n",
            " |      |  5|  50.0|  Bob|NULL|\n",
            " |      | 50|  50.0|  Tom|NULL|\n",
            " |      | 50|  50.0| NULL|true|\n",
            " |      +---+------+-----+----+\n",
            " |      \n",
            " |      Fill all null values with ``False`` for boolean columns.\n",
            " |      \n",
            " |      >>> df.na.fill(False).show()\n",
            " |      +----+------+-----+-----+\n",
            " |      | age|height| name| bool|\n",
            " |      +----+------+-----+-----+\n",
            " |      |  10|  80.5|Alice|false|\n",
            " |      |   5|  NULL|  Bob|false|\n",
            " |      |NULL|  NULL|  Tom|false|\n",
            " |      |NULL|  NULL| NULL| true|\n",
            " |      +----+------+-----+-----+\n",
            " |      \n",
            " |      Fill all null values with to 50 and \"unknown\" for 'age' and 'name' column respectively.\n",
            " |      \n",
            " |      >>> df.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
            " |      +---+------+-------+----+\n",
            " |      |age|height|   name|bool|\n",
            " |      +---+------+-------+----+\n",
            " |      | 10|  80.5|  Alice|NULL|\n",
            " |      |  5|  NULL|    Bob|NULL|\n",
            " |      | 50|  NULL|    Tom|NULL|\n",
            " |      | 50|  NULL|unknown|true|\n",
            " |      +---+------+-------+----+\n",
            " |  \n",
            " |  filter(self, condition: 'ColumnOrName') -> 'DataFrame'\n",
            " |      Filters rows using the given condition.\n",
            " |      \n",
            " |      :func:`where` is an alias for :func:`filter`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      condition : :class:`Column` or str\n",
            " |          a :class:`Column` of :class:`types.BooleanType`\n",
            " |          or a string of SQL expressions.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Filtered DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      \n",
            " |      Filter by :class:`Column` instances.\n",
            " |      \n",
            " |      >>> df.filter(df.age > 3).show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |  5| Bob|\n",
            " |      +---+----+\n",
            " |      >>> df.where(df.age == 2).show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  2|Alice|\n",
            " |      +---+-----+\n",
            " |      \n",
            " |      Filter by SQL expression in a string.\n",
            " |      \n",
            " |      >>> df.filter(\"age > 3\").show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      |  5| Bob|\n",
            " |      +---+----+\n",
            " |      >>> df.where(\"age = 2\").show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  2|Alice|\n",
            " |      +---+-----+\n",
            " |  \n",
            " |  first(self) -> Optional[pyspark.sql.types.Row]\n",
            " |      Returns the first row as a :class:`Row`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`Row`\n",
            " |          First row if :class:`DataFrame` is not empty, otherwise ``None``.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.first()\n",
            " |      Row(age=2, name='Alice')\n",
            " |  \n",
            " |  foreach(self, f: Callable[[pyspark.sql.types.Row], NoneType]) -> None\n",
            " |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
            " |      \n",
            " |      This is a shorthand for ``df.rdd.foreach()``.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      f : function\n",
            " |          A function that accepts one parameter which will\n",
            " |          receive each row to process.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> def func(person):\n",
            " |      ...     print(person.name)\n",
            " |      ...\n",
            " |      >>> df.foreach(func)\n",
            " |  \n",
            " |  foreachPartition(self, f: Callable[[Iterator[pyspark.sql.types.Row]], NoneType]) -> None\n",
            " |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
            " |      \n",
            " |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      f : function\n",
            " |          A function that accepts one parameter which will receive\n",
            " |          each partition to process.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> def func(itr):\n",
            " |      ...     for person in itr:\n",
            " |      ...         print(person.name)\n",
            " |      ...\n",
            " |      >>> df.foreachPartition(func)\n",
            " |  \n",
            " |  freqItems(self, cols: Union[List[str], Tuple[str]], support: Optional[float] = None) -> 'DataFrame'\n",
            " |      Finding frequent items for columns, possibly with false positives. Using the\n",
            " |      frequent element count algorithm described in\n",
            " |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
            " |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : list or tuple\n",
            " |          Names of the columns to calculate frequent items for as a list or tuple of\n",
            " |          strings.\n",
            " |      support : float, optional\n",
            " |          The frequency with which to consider an item 'frequent'. Default is 1%.\n",
            " |          The support must be greater than 1e-4.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with frequent items.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This function is meant for exploratory data analysis, as we make no\n",
            " |      guarantee about the backward compatibility of the schema of the resulting\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
            " |      >>> df.freqItems([\"c1\", \"c2\"]).show()  # doctest: +SKIP\n",
            " |      +------------+------------+\n",
            " |      |c1_freqItems|c2_freqItems|\n",
            " |      +------------+------------+\n",
            " |      |   [4, 1, 3]| [8, 11, 10]|\n",
            " |      +------------+------------+\n",
            " |  \n",
            " |  groupBy(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
            " |      Groups the :class:`DataFrame` using the specified columns,\n",
            " |      so we can run aggregation on them. See :class:`GroupedData`\n",
            " |      for all the available aggregate functions.\n",
            " |      \n",
            " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : list, str or :class:`Column`\n",
            " |          columns to group by.\n",
            " |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
            " |          or list of them.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`GroupedData`\n",
            " |          Grouped data by given columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (2, \"Bob\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      \n",
            " |      Empty grouping columns triggers a global aggregation.\n",
            " |      \n",
            " |      >>> df.groupBy().avg().show()\n",
            " |      +--------+\n",
            " |      |avg(age)|\n",
            " |      +--------+\n",
            " |      |    2.75|\n",
            " |      +--------+\n",
            " |      \n",
            " |      Group-by 'name', and specify a dictionary to calculate the summation of 'age'.\n",
            " |      \n",
            " |      >>> df.groupBy(\"name\").agg({\"age\": \"sum\"}).sort(\"name\").show()\n",
            " |      +-----+--------+\n",
            " |      | name|sum(age)|\n",
            " |      +-----+--------+\n",
            " |      |Alice|       2|\n",
            " |      |  Bob|       9|\n",
            " |      +-----+--------+\n",
            " |      \n",
            " |      Group-by 'name', and calculate maximum values.\n",
            " |      \n",
            " |      >>> df.groupBy(df.name).max().sort(\"name\").show()\n",
            " |      +-----+--------+\n",
            " |      | name|max(age)|\n",
            " |      +-----+--------+\n",
            " |      |Alice|       2|\n",
            " |      |  Bob|       5|\n",
            " |      +-----+--------+\n",
            " |      \n",
            " |      Group-by 'name' and 'age', and calculate the number of rows in each group.\n",
            " |      \n",
            " |      >>> df.groupBy([\"name\", df.age]).count().sort(\"name\", \"age\").show()\n",
            " |      +-----+---+-----+\n",
            " |      | name|age|count|\n",
            " |      +-----+---+-----+\n",
            " |      |Alice|  2|    1|\n",
            " |      |  Bob|  2|    2|\n",
            " |      |  Bob|  5|    1|\n",
            " |      +-----+---+-----+\n",
            " |  \n",
            " |  groupby = groupBy(self, *cols)\n",
            " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
            " |      \n",
            " |      .. versionadded:: 1.4\n",
            " |  \n",
            " |  head(self, n: Optional[int] = None) -> Union[pyspark.sql.types.Row, NoneType, List[pyspark.sql.types.Row]]\n",
            " |      Returns the first ``n`` rows.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting array is expected\n",
            " |      to be small, as all the data is loaded into the driver's memory.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      n : int, optional\n",
            " |          default 1. Number of rows to return.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      If n is greater than 1, return a list of :class:`Row`.\n",
            " |      If n is 1, return a single Row.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.head()\n",
            " |      Row(age=2, name='Alice')\n",
            " |      >>> df.head(1)\n",
            " |      [Row(age=2, name='Alice')]\n",
            " |  \n",
            " |  hint(self, name: str, *parameters: Union[ForwardRef('PrimitiveType'), List[ForwardRef('PrimitiveType')]]) -> 'DataFrame'\n",
            " |      Specifies some hint on the current :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 2.2.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          A name of the hint.\n",
            " |      parameters : str, list, float or int\n",
            " |          Optional parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Hinted DataFrame\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
            " |      >>> df.join(df2, \"name\").explain()  # doctest: +SKIP\n",
            " |      == Physical Plan ==\n",
            " |      ...\n",
            " |      ... +- SortMergeJoin ...\n",
            " |      ...\n",
            " |      \n",
            " |      Explicitly trigger the broadcast hashjoin by providing the hint in ``df2``.\n",
            " |      \n",
            " |      >>> df.join(df2.hint(\"broadcast\"), \"name\").explain()\n",
            " |      == Physical Plan ==\n",
            " |      ...\n",
            " |      ... +- BroadcastHashJoin ...\n",
            " |      ...\n",
            " |  \n",
            " |  inputFiles(self) -> List[str]\n",
            " |      Returns a best-effort snapshot of the files that compose this :class:`DataFrame`.\n",
            " |      This method simply asks each constituent BaseRelation for its respective files and\n",
            " |      takes the union of all results. Depending on the source relations, this may not find\n",
            " |      all input files. Duplicates are removed.\n",
            " |      \n",
            " |      .. versionadded:: 3.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          List of file paths.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import tempfile\n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Write a single-row DataFrame into a JSON file\n",
            " |      ...     spark.createDataFrame(\n",
            " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
            " |      ...     ).repartition(1).write.json(d, mode=\"overwrite\")\n",
            " |      ...\n",
            " |      ...     # Read the JSON file as a DataFrame.\n",
            " |      ...     df = spark.read.format(\"json\").load(d)\n",
            " |      ...\n",
            " |      ...     # Returns the number of input files.\n",
            " |      ...     len(df.inputFiles())\n",
            " |      1\n",
            " |  \n",
            " |  intersect(self, other: 'DataFrame') -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` containing rows only in\n",
            " |      both this :class:`DataFrame` and another :class:`DataFrame`.\n",
            " |      Note that any duplicates are removed. To preserve duplicates\n",
            " |      use :func:`intersectAll`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Another :class:`DataFrame` that needs to be combined.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Combined DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This is equivalent to `INTERSECT` in SQL.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
            " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
            " |      >>> df1.intersect(df2).sort(df1.C1.desc()).show()\n",
            " |      +---+---+\n",
            " |      | C1| C2|\n",
            " |      +---+---+\n",
            " |      |  b|  3|\n",
            " |      |  a|  1|\n",
            " |      +---+---+\n",
            " |  \n",
            " |  intersectAll(self, other: 'DataFrame') -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
            " |      and another :class:`DataFrame` while preserving duplicates.\n",
            " |      \n",
            " |      This is equivalent to `INTERSECT ALL` in SQL. As standard in SQL, this function\n",
            " |      resolves columns by position (not by name).\n",
            " |      \n",
            " |      .. versionadded:: 2.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Another :class:`DataFrame` that needs to be combined.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Combined DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
            " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
            " |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
            " |      +---+---+\n",
            " |      | C1| C2|\n",
            " |      +---+---+\n",
            " |      |  a|  1|\n",
            " |      |  a|  1|\n",
            " |      |  b|  3|\n",
            " |      +---+---+\n",
            " |  \n",
            " |  isEmpty(self) -> bool\n",
            " |      Checks if the :class:`DataFrame` is empty and returns a boolean value.\n",
            " |      \n",
            " |      .. versionadded:: 3.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      bool\n",
            " |          Returns ``True`` if the DataFrame is empty, ``False`` otherwise.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      DataFrame.count : Counts the number of rows in DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      - Unlike `count()`, this method does not trigger any computation.\n",
            " |      - An empty DataFrame has no rows. It may have columns, but no data.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Example 1: Checking if an empty DataFrame is empty\n",
            " |      \n",
            " |      >>> df_empty = spark.createDataFrame([], 'a STRING')\n",
            " |      >>> df_empty.isEmpty()\n",
            " |      True\n",
            " |      \n",
            " |      Example 2: Checking if a non-empty DataFrame is empty\n",
            " |      \n",
            " |      >>> df_non_empty = spark.createDataFrame([\"a\"], 'STRING')\n",
            " |      >>> df_non_empty.isEmpty()\n",
            " |      False\n",
            " |      \n",
            " |      Example 3: Checking if a DataFrame with null values is empty\n",
            " |      \n",
            " |      >>> df_nulls = spark.createDataFrame([(None, None)], 'a STRING, b INT')\n",
            " |      >>> df_nulls.isEmpty()\n",
            " |      False\n",
            " |      \n",
            " |      Example 4: Checking if a DataFrame with no rows but with columns is empty\n",
            " |      \n",
            " |      >>> df_no_rows = spark.createDataFrame([], 'id INT, value STRING')\n",
            " |      >>> df_no_rows.isEmpty()\n",
            " |      True\n",
            " |  \n",
            " |  isLocal(self) -> bool\n",
            " |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
            " |      (without any Spark executors).\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      bool\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.sql(\"SHOW TABLES\")\n",
            " |      >>> df.isLocal()\n",
            " |      True\n",
            " |  \n",
            " |  join(self, other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame'\n",
            " |      Joins with another :class:`DataFrame`, using the given join expression.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Right side of the join\n",
            " |      on : str, list or :class:`Column`, optional\n",
            " |          a string for the join column name, a list of column names,\n",
            " |          a join expression (Column), or a list of Columns.\n",
            " |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
            " |          the column(s) must exist on both sides, and this performs an equi-join.\n",
            " |      how : str, optional\n",
            " |          default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
            " |          ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
            " |          ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
            " |          ``anti``, ``leftanti`` and ``left_anti``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Joined DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      The following performs a full outer join between ``df1`` and ``df2``.\n",
            " |      \n",
            " |      >>> from pyspark.sql import Row\n",
            " |      >>> from pyspark.sql.functions import desc\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n",
            " |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
            " |      >>> df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n",
            " |      >>> df4 = spark.createDataFrame([\n",
            " |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
            " |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
            " |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
            " |      ...     Row(age=None, height=None, name=None),\n",
            " |      ... ])\n",
            " |      \n",
            " |      Inner join on columns (default)\n",
            " |      \n",
            " |      >>> df.join(df2, 'name').select(df.name, df2.height).show()\n",
            " |      +----+------+\n",
            " |      |name|height|\n",
            " |      +----+------+\n",
            " |      | Bob|    85|\n",
            " |      +----+------+\n",
            " |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).show()\n",
            " |      +----+---+\n",
            " |      |name|age|\n",
            " |      +----+---+\n",
            " |      | Bob|  5|\n",
            " |      +----+---+\n",
            " |      \n",
            " |      Outer join for both DataFrames on the 'name' column.\n",
            " |      \n",
            " |      >>> df.join(df2, df.name == df2.name, 'outer').select(\n",
            " |      ...     df.name, df2.height).sort(desc(\"name\")).show()\n",
            " |      +-----+------+\n",
            " |      | name|height|\n",
            " |      +-----+------+\n",
            " |      |  Bob|    85|\n",
            " |      |Alice|  NULL|\n",
            " |      | NULL|    80|\n",
            " |      +-----+------+\n",
            " |      >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).show()\n",
            " |      +-----+------+\n",
            " |      | name|height|\n",
            " |      +-----+------+\n",
            " |      |  Tom|    80|\n",
            " |      |  Bob|    85|\n",
            " |      |Alice|  NULL|\n",
            " |      +-----+------+\n",
            " |      \n",
            " |      Outer join for both DataFrams with multiple columns.\n",
            " |      \n",
            " |      >>> df.join(\n",
            " |      ...     df3,\n",
            " |      ...     [df.name == df3.name, df.age == df3.age],\n",
            " |      ...     'outer'\n",
            " |      ... ).select(df.name, df3.age).show()\n",
            " |      +-----+---+\n",
            " |      | name|age|\n",
            " |      +-----+---+\n",
            " |      |Alice|  2|\n",
            " |      |  Bob|  5|\n",
            " |      +-----+---+\n",
            " |  \n",
            " |  limit(self, num: int) -> 'DataFrame'\n",
            " |      Limits the result count to the number specified.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      num : int\n",
            " |          Number of records to return. Will return this number of records\n",
            " |          or all records if the DataFrame contains less than this number of records.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Subset of the records\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.limit(1).show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      | 14| Tom|\n",
            " |      +---+----+\n",
            " |      >>> df.limit(0).show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      +---+----+\n",
            " |  \n",
            " |  localCheckpoint(self, eager: bool = True) -> 'DataFrame'\n",
            " |      Returns a locally checkpointed version of this :class:`DataFrame`. Checkpointing can be\n",
            " |      used to truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
            " |      iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
            " |      stored in the executors using the caching subsystem and therefore they are not reliable.\n",
            " |      \n",
            " |      .. versionadded:: 2.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      eager : bool, optional, default True\n",
            " |          Whether to checkpoint this :class:`DataFrame` immediately.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Checkpointed DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is experimental.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.localCheckpoint(False)\n",
            " |      DataFrame[age: bigint, name: string]\n",
            " |  \n",
            " |  melt(self, ids: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...]], values: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...], NoneType], variableColumnName: str, valueColumnName: str) -> 'DataFrame'\n",
            " |      Unpivot a DataFrame from wide format to long format, optionally leaving\n",
            " |      identifier columns set. This is the reverse to `groupBy(...).pivot(...).agg(...)`,\n",
            " |      except for the aggregation, which cannot be reversed.\n",
            " |      \n",
            " |      :func:`melt` is an alias for :func:`unpivot`.\n",
            " |      \n",
            " |      .. versionadded:: 3.4.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      ids : str, Column, tuple, list, optional\n",
            " |          Column(s) to use as identifiers. Can be a single column or column name,\n",
            " |          or a list or tuple for multiple columns.\n",
            " |      values : str, Column, tuple, list, optional\n",
            " |          Column(s) to unpivot. Can be a single column or column name, or a list or tuple\n",
            " |          for multiple columns. If not specified or empty, use all columns that\n",
            " |          are not set as `ids`.\n",
            " |      variableColumnName : str\n",
            " |          Name of the variable column.\n",
            " |      valueColumnName : str\n",
            " |          Name of the value column.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Unpivoted DataFrame.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      DataFrame.unpivot\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Supports Spark Connect.\n",
            " |  \n",
            " |  observe(self, observation: Union[ForwardRef('Observation'), str], *exprs: pyspark.sql.column.Column) -> 'DataFrame'\n",
            " |      Define (named) metrics to observe on the DataFrame. This method returns an 'observed'\n",
            " |      DataFrame that returns the same result as the input, with the following guarantees:\n",
            " |      \n",
            " |      * It will compute the defined aggregates (metrics) on all the data that is flowing through\n",
            " |          the Dataset at that point.\n",
            " |      \n",
            " |      * It will report the value of the defined aggregate columns as soon as we reach a completion\n",
            " |          point. A completion point is either the end of a query (batch mode) or the end of a\n",
            " |          streaming epoch. The value of the aggregates only reflects the data processed since\n",
            " |          the previous completion point.\n",
            " |      \n",
            " |      The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or\n",
            " |      more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that\n",
            " |      contain references to the input Dataset's columns must always be wrapped in an aggregate\n",
            " |      function.\n",
            " |      \n",
            " |      A user can observe these metrics by adding\n",
            " |      Python's :class:`~pyspark.sql.streaming.StreamingQueryListener`,\n",
            " |      Scala/Java's ``org.apache.spark.sql.streaming.StreamingQueryListener`` or Scala/Java's\n",
            " |      ``org.apache.spark.sql.util.QueryExecutionListener`` to the spark session.\n",
            " |      \n",
            " |      .. versionadded:: 3.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.5.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      observation : :class:`Observation` or str\n",
            " |          `str` to specify the name, or an :class:`Observation` instance to obtain the metric.\n",
            " |      \n",
            " |          .. versionchanged:: 3.4.0\n",
            " |             Added support for `str` in this parameter.\n",
            " |      exprs : :class:`Column`\n",
            " |          column expressions (:class:`Column`).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          the observed :class:`DataFrame`.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      When ``observation`` is :class:`Observation`, this method only supports batch queries.\n",
            " |      When ``observation`` is a string, this method works for both batch and streaming queries.\n",
            " |      Continuous execution is currently not supported yet.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      When ``observation`` is :class:`Observation`, only batch queries work as below.\n",
            " |      \n",
            " |      >>> from pyspark.sql.functions import col, count, lit, max\n",
            " |      >>> from pyspark.sql import Observation\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> observation = Observation(\"my metrics\")\n",
            " |      >>> observed_df = df.observe(observation, count(lit(1)).alias(\"count\"), max(col(\"age\")))\n",
            " |      >>> observed_df.count()\n",
            " |      2\n",
            " |      >>> observation.get\n",
            " |      {'count': 2, 'max(age)': 5}\n",
            " |      \n",
            " |      When ``observation`` is a string, streaming queries also work as below.\n",
            " |      \n",
            " |      >>> from pyspark.sql.streaming import StreamingQueryListener\n",
            " |      >>> class MyErrorListener(StreamingQueryListener):\n",
            " |      ...    def onQueryStarted(self, event):\n",
            " |      ...        pass\n",
            " |      ...\n",
            " |      ...    def onQueryProgress(self, event):\n",
            " |      ...        row = event.progress.observedMetrics.get(\"my_event\")\n",
            " |      ...        # Trigger if the number of errors exceeds 5 percent\n",
            " |      ...        num_rows = row.rc\n",
            " |      ...        num_error_rows = row.erc\n",
            " |      ...        ratio = num_error_rows / num_rows\n",
            " |      ...        if ratio > 0.05:\n",
            " |      ...            # Trigger alert\n",
            " |      ...            pass\n",
            " |      ...\n",
            " |      ...    def onQueryIdle(self, event):\n",
            " |      ...        pass\n",
            " |      ...\n",
            " |      ...    def onQueryTerminated(self, event):\n",
            " |      ...        pass\n",
            " |      ...\n",
            " |      >>> spark.streams.addListener(MyErrorListener())\n",
            " |      >>> # Observe row count (rc) and error row count (erc) in the streaming Dataset\n",
            " |      ... observed_ds = df.observe(\n",
            " |      ...     \"my_event\",\n",
            " |      ...     count(lit(1)).alias(\"rc\"),\n",
            " |      ...     count(col(\"error\")).alias(\"erc\"))  # doctest: +SKIP\n",
            " |      >>> observed_ds.writeStream.format(\"console\").start()  # doctest: +SKIP\n",
            " |  \n",
            " |  offset(self, num: int) -> 'DataFrame'\n",
            " |      Returns a new :class: `DataFrame` by skipping the first `n` rows.\n",
            " |      \n",
            " |      .. versionadded:: 3.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.5.0\n",
            " |          Supports vanilla PySpark.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      num : int\n",
            " |          Number of records to skip.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Subset of the records\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.offset(1).show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      | 23|Alice|\n",
            " |      | 16|  Bob|\n",
            " |      +---+-----+\n",
            " |      >>> df.offset(10).show()\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      +---+----+\n",
            " |  \n",
            " |  orderBy = sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
            " |  \n",
            " |  pandas_api(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
            " |      Converts the existing DataFrame into a pandas-on-Spark DataFrame.\n",
            " |      \n",
            " |      .. versionadded:: 3.2.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.5.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      If a pandas-on-Spark DataFrame is converted to a Spark DataFrame and then back\n",
            " |      to pandas-on-Spark, it will lose the index information and the original index\n",
            " |      will be turned into a normal column.\n",
            " |      \n",
            " |      This is only available if Pandas is installed and available.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      index_col: str or list of str, optional, default: None\n",
            " |          Index column of table in Spark.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`PandasOnSparkDataFrame`\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      pyspark.pandas.frame.DataFrame.to_spark\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      >>> df.pandas_api()  # doctest: +SKIP\n",
            " |         age   name\n",
            " |      0   14    Tom\n",
            " |      1   23  Alice\n",
            " |      2   16    Bob\n",
            " |      \n",
            " |      We can specify the index columns.\n",
            " |      \n",
            " |      >>> df.pandas_api(index_col=\"age\")  # doctest: +SKIP\n",
            " |            name\n",
            " |      age\n",
            " |      14     Tom\n",
            " |      23   Alice\n",
            " |      16     Bob\n",
            " |  \n",
            " |  persist(self, storageLevel: pyspark.storagelevel.StorageLevel = StorageLevel(True, True, False, True, 1)) -> 'DataFrame'\n",
            " |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
            " |      operations after the first time it is computed. This can only be used to assign\n",
            " |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
            " |      If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      storageLevel : :class:`StorageLevel`\n",
            " |          Storage level to set for persistence. Default is MEMORY_AND_DISK_DESER.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Persisted DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(1)\n",
            " |      >>> df.persist()\n",
            " |      DataFrame[id: bigint]\n",
            " |      \n",
            " |      >>> df.explain()\n",
            " |      == Physical Plan ==\n",
            " |      AdaptiveSparkPlan isFinalPlan=false\n",
            " |      +- InMemoryTableScan ...\n",
            " |      \n",
            " |      Persists the data in the disk by specifying the storage level.\n",
            " |      \n",
            " |      >>> from pyspark.storagelevel import StorageLevel\n",
            " |      >>> df.persist(StorageLevel.DISK_ONLY)\n",
            " |      DataFrame[id: bigint]\n",
            " |  \n",
            " |  printSchema(self, level: Optional[int] = None) -> None\n",
            " |      Prints out the schema in the tree format.\n",
            " |      Optionally allows to specify how many levels to print if schema is nested.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      level : int, optional, default None\n",
            " |          How many levels to print for nested schemas.\n",
            " |      \n",
            " |          .. versionchanged:: 3.5.0\n",
            " |              Added Level parameter.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.printSchema()\n",
            " |      root\n",
            " |       |-- age: long (nullable = true)\n",
            " |       |-- name: string (nullable = true)\n",
            " |      \n",
            " |      >>> df = spark.createDataFrame([(1, (2,2))], [\"a\", \"b\"])\n",
            " |      >>> df.printSchema(1)\n",
            " |      root\n",
            " |       |-- a: long (nullable = true)\n",
            " |       |-- b: struct (nullable = true)\n",
            " |      \n",
            " |      >>> df.printSchema(2)\n",
            " |      root\n",
            " |       |-- a: long (nullable = true)\n",
            " |       |-- b: struct (nullable = true)\n",
            " |       |    |-- _1: long (nullable = true)\n",
            " |       |    |-- _2: long (nullable = true)\n",
            " |  \n",
            " |  randomSplit(self, weights: List[float], seed: Optional[int] = None) -> List[ForwardRef('DataFrame')]\n",
            " |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      weights : list\n",
            " |          list of doubles as weights with which to split the :class:`DataFrame`.\n",
            " |          Weights will be normalized if they don't sum up to 1.0.\n",
            " |      seed : int, optional\n",
            " |          The seed for sampling.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          List of DataFrames.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql import Row\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
            " |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
            " |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
            " |      ...     Row(age=None, height=None, name=None),\n",
            " |      ... ])\n",
            " |      \n",
            " |      >>> splits = df.randomSplit([1.0, 2.0], 24)\n",
            " |      >>> splits[0].count()\n",
            " |      2\n",
            " |      >>> splits[1].count()\n",
            " |      2\n",
            " |  \n",
            " |  registerTempTable(self, name: str) -> None\n",
            " |      Registers this :class:`DataFrame` as a temporary table using the given name.\n",
            " |      \n",
            " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
            " |      that was used to create this :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      .. deprecated:: 2.0.0\n",
            " |          Use :meth:`DataFrame.createOrReplaceTempView` instead.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      name : str\n",
            " |          Name of the temporary table to register.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.registerTempTable(\"people\")\n",
            " |      >>> df2 = spark.sql(\"SELECT * FROM people\")\n",
            " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
            " |      True\n",
            " |      >>> spark.catalog.dropTempView(\"people\")\n",
            " |      True\n",
            " |  \n",
            " |  repartition(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
            " |      resulting :class:`DataFrame` is hash partitioned.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      numPartitions : int\n",
            " |          can be an int to specify the target number of partitions or a Column.\n",
            " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
            " |          the default number of partitions is used.\n",
            " |      cols : str or :class:`Column`\n",
            " |          partitioning columns.\n",
            " |      \n",
            " |          .. versionchanged:: 1.6.0\n",
            " |             Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
            " |             optional if partitioning columns are specified.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Repartitioned DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Repartition the data into 10 partitions.\n",
            " |      \n",
            " |      >>> df.repartition(10).rdd.getNumPartitions()\n",
            " |      10\n",
            " |      \n",
            " |      Repartition the data into 7 partitions by 'age' column.\n",
            " |      \n",
            " |      >>> df.repartition(7, \"age\").rdd.getNumPartitions()\n",
            " |      7\n",
            " |      \n",
            " |      Repartition the data into 7 partitions by 'age' and 'name columns.\n",
            " |      \n",
            " |      >>> df.repartition(3, \"name\", \"age\").rdd.getNumPartitions()\n",
            " |      3\n",
            " |  \n",
            " |  repartitionByRange(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
            " |      resulting :class:`DataFrame` is range partitioned.\n",
            " |      \n",
            " |      .. versionadded:: 2.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      numPartitions : int\n",
            " |          can be an int to specify the target number of partitions or a Column.\n",
            " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
            " |          the default number of partitions is used.\n",
            " |      cols : str or :class:`Column`\n",
            " |          partitioning columns.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Repartitioned DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      At least one partition-by expression must be specified.\n",
            " |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
            " |      \n",
            " |      Due to performance reasons this method uses sampling to estimate the ranges.\n",
            " |      Hence, the output may not be consistent, since sampling can return different values.\n",
            " |      The sample size can be controlled by the config\n",
            " |      `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Repartition the data into 2 partitions by range in 'age' column.\n",
            " |      For example, the first partition can have ``(14, \"Tom\")``, and the second\n",
            " |      partition would have ``(16, \"Bob\")`` and ``(23, \"Alice\")``.\n",
            " |      \n",
            " |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
            " |      2\n",
            " |  \n",
            " |  replace(self, to_replace: Union[ForwardRef('LiteralType'), List[ForwardRef('LiteralType')], Dict[ForwardRef('LiteralType'), ForwardRef('OptionalPrimitiveType')]], value: Union[ForwardRef('OptionalPrimitiveType'), List[ForwardRef('OptionalPrimitiveType')], pyspark._globals._NoValueType, NoneType] = <no value>, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
            " |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
            " |      aliases of each other.\n",
            " |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
            " |      or strings. Value can have None. When replacing, the new value will be cast\n",
            " |      to the type of the existing column.\n",
            " |      For numeric replacements all values to be replaced should have unique\n",
            " |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
            " |      and arbitrary replacement will be used.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      to_replace : bool, int, float, string, list or dict\n",
            " |          Value to be replaced.\n",
            " |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
            " |          must be a mapping between a value and a replacement.\n",
            " |      value : bool, int, float, string or None, optional\n",
            " |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
            " |          list, `value` should be of the same length and type as `to_replace`.\n",
            " |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
            " |          used as a replacement for each item in `to_replace`.\n",
            " |      subset : list, optional\n",
            " |          optional list of column names to consider.\n",
            " |          Columns specified in subset that do not have matching data types are ignored.\n",
            " |          For example, if `value` is a string, and subset contains a non-string column,\n",
            " |          then the non-string column is simply ignored.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with replaced values.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (10, 80, \"Alice\"),\n",
            " |      ...     (5, None, \"Bob\"),\n",
            " |      ...     (None, 10, \"Tom\"),\n",
            " |      ...     (None, None, None)],\n",
            " |      ...     schema=[\"age\", \"height\", \"name\"])\n",
            " |      \n",
            " |      Replace 10 to 20 in all columns.\n",
            " |      \n",
            " |      >>> df.na.replace(10, 20).show()\n",
            " |      +----+------+-----+\n",
            " |      | age|height| name|\n",
            " |      +----+------+-----+\n",
            " |      |  20|    80|Alice|\n",
            " |      |   5|  NULL|  Bob|\n",
            " |      |NULL|    20|  Tom|\n",
            " |      |NULL|  NULL| NULL|\n",
            " |      +----+------+-----+\n",
            " |      \n",
            " |      Replace 'Alice' to null in all columns.\n",
            " |      \n",
            " |      >>> df.na.replace('Alice', None).show()\n",
            " |      +----+------+----+\n",
            " |      | age|height|name|\n",
            " |      +----+------+----+\n",
            " |      |  10|    80|NULL|\n",
            " |      |   5|  NULL| Bob|\n",
            " |      |NULL|    10| Tom|\n",
            " |      |NULL|  NULL|NULL|\n",
            " |      +----+------+----+\n",
            " |      \n",
            " |      Replace 'Alice' to 'A', and 'Bob' to 'B' in the 'name' column.\n",
            " |      \n",
            " |      >>> df.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
            " |      +----+------+----+\n",
            " |      | age|height|name|\n",
            " |      +----+------+----+\n",
            " |      |  10|    80|   A|\n",
            " |      |   5|  NULL|   B|\n",
            " |      |NULL|    10| Tom|\n",
            " |      |NULL|  NULL|NULL|\n",
            " |      +----+------+----+\n",
            " |  \n",
            " |  rollup(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
            " |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
            " |      the specified columns, so we can run aggregation on them.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : list, str or :class:`Column`\n",
            " |          Columns to roll-up by.\n",
            " |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
            " |          or list of them.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`GroupedData`\n",
            " |          Rolled-up data by given columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
            " |      +-----+----+-----+\n",
            " |      | name| age|count|\n",
            " |      +-----+----+-----+\n",
            " |      | NULL|NULL|    2|\n",
            " |      |Alice|NULL|    1|\n",
            " |      |Alice|   2|    1|\n",
            " |      |  Bob|NULL|    1|\n",
            " |      |  Bob|   5|    1|\n",
            " |      +-----+----+-----+\n",
            " |  \n",
            " |  sameSemantics(self, other: 'DataFrame') -> bool\n",
            " |      Returns `True` when the logical query plans inside both :class:`DataFrame`\\s are equal and\n",
            " |      therefore return the same results.\n",
            " |      \n",
            " |      .. versionadded:: 3.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.5.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The equality comparison here is simplified by tolerating the cosmetic differences\n",
            " |      such as attribute names.\n",
            " |      \n",
            " |      This API can compare both :class:`DataFrame`\\s very fast but can still return\n",
            " |      `False` on the :class:`DataFrame` that return the same results, for instance, from\n",
            " |      different plans. Such false negative semantic can be useful when caching as an example.\n",
            " |      \n",
            " |      This API is a developer API.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          The other DataFrame to compare against.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      bool\n",
            " |          Whether these two DataFrames are similar.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df1 = spark.range(10)\n",
            " |      >>> df2 = spark.range(10)\n",
            " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\n",
            " |      True\n",
            " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\n",
            " |      False\n",
            " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\n",
            " |      True\n",
            " |  \n",
            " |  sample(self, withReplacement: Union[float, bool, NoneType] = None, fraction: Union[int, float, NoneType] = None, seed: Optional[int] = None) -> 'DataFrame'\n",
            " |      Returns a sampled subset of this :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      withReplacement : bool, optional\n",
            " |          Sample with replacement or not (default ``False``).\n",
            " |      fraction : float, optional\n",
            " |          Fraction of rows to generate, range [0.0, 1.0].\n",
            " |      seed : int, optional\n",
            " |          Seed for sampling (default a random seed).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Sampled rows from given DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This is not guaranteed to provide exactly the fraction specified of the total\n",
            " |      count of the given :class:`DataFrame`.\n",
            " |      \n",
            " |      `fraction` is required and, `withReplacement` and `seed` are optional.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(10)\n",
            " |      >>> df.sample(0.5, 3).count() # doctest: +SKIP\n",
            " |      7\n",
            " |      >>> df.sample(fraction=0.5, seed=3).count() # doctest: +SKIP\n",
            " |      7\n",
            " |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count() # doctest: +SKIP\n",
            " |      1\n",
            " |      >>> df.sample(1.0).count()\n",
            " |      10\n",
            " |      >>> df.sample(fraction=1.0).count()\n",
            " |      10\n",
            " |      >>> df.sample(False, fraction=1.0).count()\n",
            " |      10\n",
            " |  \n",
            " |  sampleBy(self, col: 'ColumnOrName', fractions: Dict[Any, float], seed: Optional[int] = None) -> 'DataFrame'\n",
            " |      Returns a stratified sample without replacement based on the\n",
            " |      fraction given on each stratum.\n",
            " |      \n",
            " |      .. versionadded:: 1.5.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      col : :class:`Column` or str\n",
            " |          column that defines strata\n",
            " |      \n",
            " |          .. versionchanged:: 3.0.0\n",
            " |             Added sampling by a column of :class:`Column`\n",
            " |      fractions : dict\n",
            " |          sampling fraction for each stratum. If a stratum is not\n",
            " |          specified, we treat its fraction as zero.\n",
            " |      seed : int, optional\n",
            " |          random seed\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      a new :class:`DataFrame` that represents the stratified sample\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.functions import col\n",
            " |      >>> dataset = spark.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
            " |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
            " |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
            " |      +---+-----+\n",
            " |      |key|count|\n",
            " |      +---+-----+\n",
            " |      |  0|    3|\n",
            " |      |  1|    6|\n",
            " |      +---+-----+\n",
            " |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
            " |      33\n",
            " |  \n",
            " |  select(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
            " |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : str, :class:`Column`, or list\n",
            " |          column names (string) or expressions (:class:`Column`).\n",
            " |          If one of the column names is '*', that column is expanded to include all columns\n",
            " |          in the current :class:`DataFrame`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          A DataFrame with subset (or all) of columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      \n",
            " |      Select all columns in the DataFrame.\n",
            " |      \n",
            " |      >>> df.select('*').show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  2|Alice|\n",
            " |      |  5|  Bob|\n",
            " |      +---+-----+\n",
            " |      \n",
            " |      Select a column with other expressions in the DataFrame.\n",
            " |      \n",
            " |      >>> df.select(df.name, (df.age + 10).alias('age')).show()\n",
            " |      +-----+---+\n",
            " |      | name|age|\n",
            " |      +-----+---+\n",
            " |      |Alice| 12|\n",
            " |      |  Bob| 15|\n",
            " |      +-----+---+\n",
            " |  \n",
            " |  selectExpr(self, *expr: Union[str, List[str]]) -> 'DataFrame'\n",
            " |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
            " |      \n",
            " |      This is a variant of :func:`select` that accepts SQL expressions.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          A DataFrame with new/old columns transformed by expressions.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").show()\n",
            " |      +---------+--------+\n",
            " |      |(age * 2)|abs(age)|\n",
            " |      +---------+--------+\n",
            " |      |        4|       2|\n",
            " |      |       10|       5|\n",
            " |      +---------+--------+\n",
            " |  \n",
            " |  semanticHash(self) -> int\n",
            " |      Returns a hash code of the logical query plan against this :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 3.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.5.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Unlike the standard hash code, the hash is calculated against the query plan\n",
            " |      simplified by tolerating the cosmetic differences such as attribute names.\n",
            " |      \n",
            " |      This API is a developer API.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      int\n",
            " |          Hash value.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> spark.range(10).selectExpr(\"id as col0\").semanticHash()  # doctest: +SKIP\n",
            " |      1855039936\n",
            " |      >>> spark.range(10).selectExpr(\"id as col1\").semanticHash()  # doctest: +SKIP\n",
            " |      1855039936\n",
            " |  \n",
            " |  show(self, n: int = 20, truncate: Union[bool, int] = True, vertical: bool = False) -> None\n",
            " |      Prints the first ``n`` rows to the console.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      n : int, optional\n",
            " |          Number of rows to show.\n",
            " |      truncate : bool or int, optional\n",
            " |          If set to ``True``, truncate strings longer than 20 chars by default.\n",
            " |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
            " |          and align cells right.\n",
            " |      vertical : bool, optional\n",
            " |          If set to ``True``, print output rows vertically (one line\n",
            " |          per column value).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Show only top 2 rows.\n",
            " |      \n",
            " |      >>> df.show(2)\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      | 14|  Tom|\n",
            " |      | 23|Alice|\n",
            " |      +---+-----+\n",
            " |      only showing top 2 rows\n",
            " |      \n",
            " |      Show :class:`DataFrame` where the maximum number of characters is 3.\n",
            " |      \n",
            " |      >>> df.show(truncate=3)\n",
            " |      +---+----+\n",
            " |      |age|name|\n",
            " |      +---+----+\n",
            " |      | 14| Tom|\n",
            " |      | 23| Ali|\n",
            " |      | 16| Bob|\n",
            " |      +---+----+\n",
            " |      \n",
            " |      Show :class:`DataFrame` vertically.\n",
            " |      \n",
            " |      >>> df.show(vertical=True)\n",
            " |      -RECORD 0-----\n",
            " |      age  | 14\n",
            " |      name | Tom\n",
            " |      -RECORD 1-----\n",
            " |      age  | 23\n",
            " |      name | Alice\n",
            " |      -RECORD 2-----\n",
            " |      age  | 16\n",
            " |      name | Bob\n",
            " |  \n",
            " |  sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : str, list, or :class:`Column`, optional\n",
            " |           list of :class:`Column` or column names to sort by.\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      ascending : bool or list, optional, default True\n",
            " |          boolean or list of boolean.\n",
            " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
            " |          If a list is specified, the length of the list must equal the length of the `cols`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Sorted DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.functions import desc, asc\n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      \n",
            " |      Sort the DataFrame in ascending order.\n",
            " |      \n",
            " |      >>> df.sort(asc(\"age\")).show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  2|Alice|\n",
            " |      |  5|  Bob|\n",
            " |      +---+-----+\n",
            " |      \n",
            " |      Sort the DataFrame in descending order.\n",
            " |      \n",
            " |      >>> df.sort(df.age.desc()).show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  5|  Bob|\n",
            " |      |  2|Alice|\n",
            " |      +---+-----+\n",
            " |      >>> df.orderBy(df.age.desc()).show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  5|  Bob|\n",
            " |      |  2|Alice|\n",
            " |      +---+-----+\n",
            " |      >>> df.sort(\"age\", ascending=False).show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  5|  Bob|\n",
            " |      |  2|Alice|\n",
            " |      +---+-----+\n",
            " |      \n",
            " |      Specify multiple columns\n",
            " |      \n",
            " |      >>> df = spark.createDataFrame([\n",
            " |      ...     (2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.orderBy(desc(\"age\"), \"name\").show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  5|  Bob|\n",
            " |      |  2|Alice|\n",
            " |      |  2|  Bob|\n",
            " |      +---+-----+\n",
            " |      \n",
            " |      Specify multiple columns for sorting order at `ascending`.\n",
            " |      \n",
            " |      >>> df.orderBy([\"age\", \"name\"], ascending=[False, False]).show()\n",
            " |      +---+-----+\n",
            " |      |age| name|\n",
            " |      +---+-----+\n",
            " |      |  5|  Bob|\n",
            " |      |  2|  Bob|\n",
            " |      |  2|Alice|\n",
            " |      +---+-----+\n",
            " |  \n",
            " |  sortWithinPartitions(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
            " |      \n",
            " |      .. versionadded:: 1.6.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cols : str, list or :class:`Column`, optional\n",
            " |          list of :class:`Column` or column names to sort by.\n",
            " |      \n",
            " |      Other Parameters\n",
            " |      ----------------\n",
            " |      ascending : bool or list, optional, default True\n",
            " |          boolean or list of boolean.\n",
            " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
            " |          If a list is specified, the length of the list must equal the length of the `cols`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame sorted by partitions.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.sortWithinPartitions(\"age\", ascending=False)\n",
            " |      DataFrame[age: bigint, name: string]\n",
            " |  \n",
            " |  subtract(self, other: 'DataFrame') -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
            " |      but not in another :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Another :class:`DataFrame` that needs to be subtracted.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Subtracted DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
            " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
            " |      >>> df1.subtract(df2).show()\n",
            " |      +---+---+\n",
            " |      | C1| C2|\n",
            " |      +---+---+\n",
            " |      |  c|  4|\n",
            " |      +---+---+\n",
            " |  \n",
            " |  summary(self, *statistics: str) -> 'DataFrame'\n",
            " |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
            " |      - count\n",
            " |      - mean\n",
            " |      - stddev\n",
            " |      - min\n",
            " |      - max\n",
            " |      - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n",
            " |      \n",
            " |      If no statistics are given, this function computes count, mean, stddev, min,\n",
            " |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
            " |      \n",
            " |      .. versionadded:: 2.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      statistics : str, optional\n",
            " |           Column names to calculate statistics by (default All columns).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          A new DataFrame that provides statistics for the given DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This function is meant for exploratory data analysis, as we make no\n",
            " |      guarantee about the backward compatibility of the schema of the resulting\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
            " |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
            " |      ... )\n",
            " |      >>> df.select(\"age\", \"weight\", \"height\").summary().show()\n",
            " |      +-------+----+------------------+-----------------+\n",
            " |      |summary| age|            weight|           height|\n",
            " |      +-------+----+------------------+-----------------+\n",
            " |      |  count|   3|                 3|                3|\n",
            " |      |   mean|12.0| 40.73333333333333|            145.0|\n",
            " |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
            " |      |    min|  11|              37.8|            142.2|\n",
            " |      |    25%|  11|              37.8|            142.2|\n",
            " |      |    50%|  12|              40.3|            142.3|\n",
            " |      |    75%|  13|              44.1|            150.5|\n",
            " |      |    max|  13|              44.1|            150.5|\n",
            " |      +-------+----+------------------+-----------------+\n",
            " |      \n",
            " |      >>> df.select(\"age\", \"weight\", \"height\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
            " |      +-------+---+------+------+\n",
            " |      |summary|age|weight|height|\n",
            " |      +-------+---+------+------+\n",
            " |      |  count|  3|     3|     3|\n",
            " |      |    min| 11|  37.8| 142.2|\n",
            " |      |    25%| 11|  37.8| 142.2|\n",
            " |      |    75%| 13|  44.1| 150.5|\n",
            " |      |    max| 13|  44.1| 150.5|\n",
            " |      +-------+---+------+------+\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      DataFrame.display\n",
            " |  \n",
            " |  tail(self, num: int) -> List[pyspark.sql.types.Row]\n",
            " |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
            " |      \n",
            " |      Running tail requires moving data into the application's driver process, and doing so with\n",
            " |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
            " |      \n",
            " |      .. versionadded:: 3.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      num : int\n",
            " |          Number of records to return. Will return this number of records\n",
            " |          or all records if the DataFrame contains less than this number of records.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          List of rows\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      >>> df.tail(2)\n",
            " |      [Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
            " |  \n",
            " |  take(self, num: int) -> List[pyspark.sql.types.Row]\n",
            " |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      num : int\n",
            " |          Number of records to return. Will return this number of records\n",
            " |          or all records if the DataFrame contains less than this number of records..\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          List of rows\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Return the first 2 rows of the :class:`DataFrame`.\n",
            " |      \n",
            " |      >>> df.take(2)\n",
            " |      [Row(age=14, name='Tom'), Row(age=23, name='Alice')]\n",
            " |  \n",
            " |  to(self, schema: pyspark.sql.types.StructType) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` where each row is reconciled to match the specified\n",
            " |      schema.\n",
            " |      \n",
            " |      .. versionadded:: 3.4.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      schema : :class:`StructType`\n",
            " |          Specified schema.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Reconciled DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      * Reorder columns and/or inner fields by name to match the specified schema.\n",
            " |      \n",
            " |      * Project away columns and/or inner fields that are not needed by the specified schema.\n",
            " |          Missing columns and/or inner fields (present in the specified schema but not input\n",
            " |          DataFrame) lead to failures.\n",
            " |      \n",
            " |      * Cast the columns and/or inner fields to match the data types in the specified schema,\n",
            " |          if the types are compatible, e.g., numeric to numeric (error if overflows), but\n",
            " |          not string to int.\n",
            " |      \n",
            " |      * Carry over the metadata from the specified schema, while the columns and/or inner fields\n",
            " |          still keep their own metadata if not overwritten by the specified schema.\n",
            " |      \n",
            " |      * Fail if the nullability is not compatible. For example, the column and/or inner field\n",
            " |          is nullable but the specified schema requires them to be not nullable.\n",
            " |      \n",
            " |      Supports Spark Connect.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.types import StructField, StringType\n",
            " |      >>> df = spark.createDataFrame([(\"a\", 1)], [\"i\", \"j\"])\n",
            " |      >>> df.schema\n",
            " |      StructType([StructField('i', StringType(), True), StructField('j', LongType(), True)])\n",
            " |      \n",
            " |      >>> schema = StructType([StructField(\"j\", StringType()), StructField(\"i\", StringType())])\n",
            " |      >>> df2 = df.to(schema)\n",
            " |      >>> df2.schema\n",
            " |      StructType([StructField('j', StringType(), True), StructField('i', StringType(), True)])\n",
            " |      >>> df2.show()\n",
            " |      +---+---+\n",
            " |      |  j|  i|\n",
            " |      +---+---+\n",
            " |      |  1|  a|\n",
            " |      +---+---+\n",
            " |  \n",
            " |  toDF(self, *cols: str) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` that with new specified column names\n",
            " |      \n",
            " |      .. versionadded:: 1.6.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      *cols : tuple\n",
            " |          a tuple of string new column name. The length of the\n",
            " |          list needs to be the same as the number of columns in the initial\n",
            " |          :class:`DataFrame`\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with new column names.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"),\n",
            " |      ...     (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.toDF('f1', 'f2').show()\n",
            " |      +---+-----+\n",
            " |      | f1|   f2|\n",
            " |      +---+-----+\n",
            " |      | 14|  Tom|\n",
            " |      | 23|Alice|\n",
            " |      | 16|  Bob|\n",
            " |      +---+-----+\n",
            " |  \n",
            " |  toJSON(self, use_unicode: bool = True) -> pyspark.rdd.RDD[str]\n",
            " |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
            " |      \n",
            " |      Each row is turned into a JSON document as one element in the returned RDD.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      use_unicode : bool, optional, default True\n",
            " |          Whether to convert to unicode or not.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.toJSON().first()\n",
            " |      '{\"age\":2,\"name\":\"Alice\"}'\n",
            " |  \n",
            " |  toLocalIterator(self, prefetchPartitions: bool = False) -> Iterator[pyspark.sql.types.Row]\n",
            " |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
            " |      The iterator will consume as much memory as the largest partition in this\n",
            " |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
            " |      partitions.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      prefetchPartitions : bool, optional\n",
            " |          If Spark should pre-fetch the next partition before it is needed.\n",
            " |      \n",
            " |          .. versionchanged:: 3.4.0\n",
            " |              This argument does not take effect for Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      Iterator\n",
            " |          Iterator of rows.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> list(df.toLocalIterator())\n",
            " |      [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
            " |  \n",
            " |  to_koalas(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
            " |      # Keep to_koalas for backward compatibility for now.\n",
            " |  \n",
            " |  to_pandas_on_spark(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
            " |      # Keep to_pandas_on_spark for backward compatibility for now.\n",
            " |  \n",
            " |  transform(self, func: Callable[..., ForwardRef('DataFrame')], *args: Any, **kwargs: Any) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
            " |      \n",
            " |      .. versionadded:: 3.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      func : function\n",
            " |          a function that takes and returns a :class:`DataFrame`.\n",
            " |      *args\n",
            " |          Positional arguments to pass to func.\n",
            " |      \n",
            " |          .. versionadded:: 3.3.0\n",
            " |      **kwargs\n",
            " |          Keyword arguments to pass to func.\n",
            " |      \n",
            " |          .. versionadded:: 3.3.0\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Transformed DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.functions import col\n",
            " |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
            " |      >>> def cast_all_to_int(input_df):\n",
            " |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
            " |      ...\n",
            " |      >>> def sort_columns_asc(input_df):\n",
            " |      ...     return input_df.select(*sorted(input_df.columns))\n",
            " |      ...\n",
            " |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
            " |      +-----+---+\n",
            " |      |float|int|\n",
            " |      +-----+---+\n",
            " |      |    1|  1|\n",
            " |      |    2|  2|\n",
            " |      +-----+---+\n",
            " |      \n",
            " |      >>> def add_n(input_df, n):\n",
            " |      ...     return input_df.select([(col(col_name) + n).alias(col_name)\n",
            " |      ...                             for col_name in input_df.columns])\n",
            " |      >>> df.transform(add_n, 1).transform(add_n, n=10).show()\n",
            " |      +---+-----+\n",
            " |      |int|float|\n",
            " |      +---+-----+\n",
            " |      | 12| 12.0|\n",
            " |      | 13| 13.0|\n",
            " |      +---+-----+\n",
            " |  \n",
            " |  union(self, other: 'DataFrame') -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` containing the union of rows in this and another\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Another :class:`DataFrame` that needs to be unioned.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          A new :class:`DataFrame` containing the combined rows with corresponding columns.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      DataFrame.unionAll\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method performs a SQL-style set union of the rows from both `DataFrame` objects,\n",
            " |      with no automatic deduplication of elements.\n",
            " |      \n",
            " |      Use the `distinct()` method to perform deduplication of rows.\n",
            " |      \n",
            " |      The method resolves columns by position (not by name), following the standard behavior\n",
            " |      in SQL.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Example 1: Combining two DataFrames with the same schema\n",
            " |      \n",
            " |      >>> df1 = spark.createDataFrame([(1, 'A'), (2, 'B')], ['id', 'value'])\n",
            " |      >>> df2 = spark.createDataFrame([(3, 'C'), (4, 'D')], ['id', 'value'])\n",
            " |      >>> df3 = df1.union(df2)\n",
            " |      >>> df3.show()\n",
            " |      +---+-----+\n",
            " |      | id|value|\n",
            " |      +---+-----+\n",
            " |      |  1|    A|\n",
            " |      |  2|    B|\n",
            " |      |  3|    C|\n",
            " |      |  4|    D|\n",
            " |      +---+-----+\n",
            " |      \n",
            " |      Example 2: Combining two DataFrames with different schemas\n",
            " |      \n",
            " |      >>> from pyspark.sql.functions import lit\n",
            " |      >>> df1 = spark.createDataFrame([(\"Alice\", 1), (\"Bob\", 2)], [\"name\", \"id\"])\n",
            " |      >>> df2 = spark.createDataFrame([(3, \"Charlie\"), (4, \"Dave\")], [\"id\", \"name\"])\n",
            " |      >>> df1 = df1.withColumn(\"age\", lit(30))\n",
            " |      >>> df2 = df2.withColumn(\"age\", lit(40))\n",
            " |      >>> df3 = df1.union(df2)\n",
            " |      >>> df3.show()\n",
            " |      +-----+-------+---+\n",
            " |      | name|     id|age|\n",
            " |      +-----+-------+---+\n",
            " |      |Alice|      1| 30|\n",
            " |      |  Bob|      2| 30|\n",
            " |      |    3|Charlie| 40|\n",
            " |      |    4|   Dave| 40|\n",
            " |      +-----+-------+---+\n",
            " |      \n",
            " |      Example 3: Combining two DataFrames with mismatched columns\n",
            " |      \n",
            " |      >>> df1 = spark.createDataFrame([(1, 2)], [\"A\", \"B\"])\n",
            " |      >>> df2 = spark.createDataFrame([(3, 4)], [\"C\", \"D\"])\n",
            " |      >>> df3 = df1.union(df2)\n",
            " |      >>> df3.show()\n",
            " |      +---+---+\n",
            " |      |  A|  B|\n",
            " |      +---+---+\n",
            " |      |  1|  2|\n",
            " |      |  3|  4|\n",
            " |      +---+---+\n",
            " |      \n",
            " |      Example 4: Combining duplicate rows from two different DataFrames\n",
            " |      \n",
            " |      >>> df1 = spark.createDataFrame([(1, 'A'), (2, 'B'), (3, 'C')], ['id', 'value'])\n",
            " |      >>> df2 = spark.createDataFrame([(3, 'C'), (4, 'D')], ['id', 'value'])\n",
            " |      >>> df3 = df1.union(df2).distinct().sort(\"id\")\n",
            " |      >>> df3.show()\n",
            " |      +---+-----+\n",
            " |      | id|value|\n",
            " |      +---+-----+\n",
            " |      |  1|    A|\n",
            " |      |  2|    B|\n",
            " |      |  3|    C|\n",
            " |      |  4|    D|\n",
            " |      +---+-----+\n",
            " |  \n",
            " |  unionAll(self, other: 'DataFrame') -> 'DataFrame'\n",
            " |      Return a new :class:`DataFrame` containing the union of rows in this and another\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Another :class:`DataFrame` that needs to be combined\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          A new :class:`DataFrame` containing combined rows from both dataframes.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method combines all rows from both `DataFrame` objects with no automatic\n",
            " |      deduplication of elements.\n",
            " |      \n",
            " |      Use the `distinct()` method to perform deduplication of rows.\n",
            " |      \n",
            " |      :func:`unionAll` is an alias to :func:`union`\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      DataFrame.union\n",
            " |  \n",
            " |  unionByName(self, other: 'DataFrame', allowMissingColumns: bool = False) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` containing union of rows in this and another\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      This method performs a union operation on both input DataFrames, resolving columns by\n",
            " |      name (rather than position). When `allowMissingColumns` is True, missing columns will\n",
            " |      be filled with null.\n",
            " |      \n",
            " |      .. versionadded:: 2.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other : :class:`DataFrame`\n",
            " |          Another :class:`DataFrame` that needs to be combined.\n",
            " |      allowMissingColumns : bool, optional, default False\n",
            " |         Specify whether to allow missing columns.\n",
            " |      \n",
            " |         .. versionadded:: 3.1.0\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          A new :class:`DataFrame` containing the combined rows with corresponding\n",
            " |          columns of the two given DataFrames.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Example 1: Union of two DataFrames with same columns in different order.\n",
            " |      \n",
            " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
            " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
            " |      >>> df1.unionByName(df2).show()\n",
            " |      +----+----+----+\n",
            " |      |col0|col1|col2|\n",
            " |      +----+----+----+\n",
            " |      |   1|   2|   3|\n",
            " |      |   6|   4|   5|\n",
            " |      +----+----+----+\n",
            " |      \n",
            " |      Example 2: Union with missing columns and setting `allowMissingColumns=True`.\n",
            " |      \n",
            " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
            " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n",
            " |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
            " |      +----+----+----+----+\n",
            " |      |col0|col1|col2|col3|\n",
            " |      +----+----+----+----+\n",
            " |      |   1|   2|   3|NULL|\n",
            " |      |NULL|   4|   5|   6|\n",
            " |      +----+----+----+----+\n",
            " |      \n",
            " |      Example 3: Union of two DataFrames with few common columns.\n",
            " |      \n",
            " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
            " |      >>> df2 = spark.createDataFrame([[4, 5, 6, 7]], [\"col1\", \"col2\", \"col3\", \"col4\"])\n",
            " |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
            " |      +----+----+----+----+----+\n",
            " |      |col0|col1|col2|col3|col4|\n",
            " |      +----+----+----+----+----+\n",
            " |      |   1|   2|   3|NULL|NULL|\n",
            " |      |NULL|   4|   5|   6|   7|\n",
            " |      +----+----+----+----+----+\n",
            " |      \n",
            " |      Example 4: Union of two DataFrames with completely different columns.\n",
            " |      \n",
            " |      >>> df1 = spark.createDataFrame([[0, 1, 2]], [\"col0\", \"col1\", \"col2\"])\n",
            " |      >>> df2 = spark.createDataFrame([[3, 4, 5]], [\"col3\", \"col4\", \"col5\"])\n",
            " |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
            " |      +----+----+----+----+----+----+\n",
            " |      |col0|col1|col2|col3|col4|col5|\n",
            " |      +----+----+----+----+----+----+\n",
            " |      |   0|   1|   2|NULL|NULL|NULL|\n",
            " |      |NULL|NULL|NULL|   3|   4|   5|\n",
            " |      +----+----+----+----+----+----+\n",
            " |  \n",
            " |  unpersist(self, blocking: bool = False) -> 'DataFrame'\n",
            " |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
            " |      memory and disk.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      blocking : bool\n",
            " |          Whether to block until all blocks are deleted.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Unpersisted DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(1)\n",
            " |      >>> df.persist()\n",
            " |      DataFrame[id: bigint]\n",
            " |      >>> df.unpersist()\n",
            " |      DataFrame[id: bigint]\n",
            " |      >>> df = spark.range(1)\n",
            " |      >>> df.unpersist(True)\n",
            " |      DataFrame[id: bigint]\n",
            " |  \n",
            " |  unpivot(self, ids: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...]], values: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...], NoneType], variableColumnName: str, valueColumnName: str) -> 'DataFrame'\n",
            " |      Unpivot a DataFrame from wide format to long format, optionally leaving\n",
            " |      identifier columns set. This is the reverse to `groupBy(...).pivot(...).agg(...)`,\n",
            " |      except for the aggregation, which cannot be reversed.\n",
            " |      \n",
            " |      This function is useful to massage a DataFrame into a format where some\n",
            " |      columns are identifier columns (\"ids\"), while all other columns (\"values\")\n",
            " |      are \"unpivoted\" to the rows, leaving just two non-id columns, named as given\n",
            " |      by `variableColumnName` and `valueColumnName`.\n",
            " |      \n",
            " |      When no \"id\" columns are given, the unpivoted DataFrame consists of only the\n",
            " |      \"variable\" and \"value\" columns.\n",
            " |      \n",
            " |      The `values` columns must not be empty so at least one value must be given to be unpivoted.\n",
            " |      When `values` is `None`, all non-id columns will be unpivoted.\n",
            " |      \n",
            " |      All \"value\" columns must share a least common data type. Unless they are the same data type,\n",
            " |      all \"value\" columns are cast to the nearest common data type. For instance, types\n",
            " |      `IntegerType` and `LongType` are cast to `LongType`, while `IntegerType` and `StringType`\n",
            " |      do not have a common data type and `unpivot` fails.\n",
            " |      \n",
            " |      .. versionadded:: 3.4.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      ids : str, Column, tuple, list\n",
            " |          Column(s) to use as identifiers. Can be a single column or column name,\n",
            " |          or a list or tuple for multiple columns.\n",
            " |      values : str, Column, tuple, list, optional\n",
            " |          Column(s) to unpivot. Can be a single column or column name, or a list or tuple\n",
            " |          for multiple columns. If specified, must not be empty. If not specified, uses all\n",
            " |          columns that are not set as `ids`.\n",
            " |      variableColumnName : str\n",
            " |          Name of the variable column.\n",
            " |      valueColumnName : str\n",
            " |          Name of the value column.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Unpivoted DataFrame.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Supports Spark Connect.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(1, 11, 1.1), (2, 12, 1.2)],\n",
            " |      ...     [\"id\", \"int\", \"double\"],\n",
            " |      ... )\n",
            " |      >>> df.show()\n",
            " |      +---+---+------+\n",
            " |      | id|int|double|\n",
            " |      +---+---+------+\n",
            " |      |  1| 11|   1.1|\n",
            " |      |  2| 12|   1.2|\n",
            " |      +---+---+------+\n",
            " |      \n",
            " |      >>> df.unpivot(\"id\", [\"int\", \"double\"], \"var\", \"val\").show()\n",
            " |      +---+------+----+\n",
            " |      | id|   var| val|\n",
            " |      +---+------+----+\n",
            " |      |  1|   int|11.0|\n",
            " |      |  1|double| 1.1|\n",
            " |      |  2|   int|12.0|\n",
            " |      |  2|double| 1.2|\n",
            " |      +---+------+----+\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      DataFrame.melt\n",
            " |  \n",
            " |  where = filter(self, condition)\n",
            " |      :func:`where` is an alias for :func:`filter`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3\n",
            " |  \n",
            " |  withColumn(self, colName: str, col: pyspark.sql.column.Column) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
            " |      existing column that has the same name.\n",
            " |      \n",
            " |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
            " |      a column from some other :class:`DataFrame` will raise an error.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      colName : str\n",
            " |          string, name of the new column.\n",
            " |      col : :class:`Column`\n",
            " |          a :class:`Column` expression for the new column.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with new or replaced column.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method introduces a projection internally. Therefore, calling it multiple\n",
            " |      times, for instance, via loops in order to add multiple columns can generate big\n",
            " |      plans which can cause performance issues and even `StackOverflowException`.\n",
            " |      To avoid this, use :func:`select` with multiple columns at once.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.withColumn('age2', df.age + 2).show()\n",
            " |      +---+-----+----+\n",
            " |      |age| name|age2|\n",
            " |      +---+-----+----+\n",
            " |      |  2|Alice|   4|\n",
            " |      |  5|  Bob|   7|\n",
            " |      +---+-----+----+\n",
            " |  \n",
            " |  withColumnRenamed(self, existing: str, new: str) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
            " |      This is a no-op if the schema doesn't contain the given column name.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      existing : str\n",
            " |          string, name of the existing column to rename.\n",
            " |      new : str\n",
            " |          string, new name of the column.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with renamed column.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.withColumnRenamed('age', 'age2').show()\n",
            " |      +----+-----+\n",
            " |      |age2| name|\n",
            " |      +----+-----+\n",
            " |      |   2|Alice|\n",
            " |      |   5|  Bob|\n",
            " |      +----+-----+\n",
            " |  \n",
            " |  withColumns(self, *colsMap: Dict[str, pyspark.sql.column.Column]) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` by adding multiple columns or replacing the\n",
            " |      existing columns that have the same names.\n",
            " |      \n",
            " |      The colsMap is a map of column name and column, the column must only refer to attributes\n",
            " |      supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n",
            " |      \n",
            " |      .. versionadded:: 3.3.0\n",
            " |         Added support for multiple columns adding\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      colsMap : dict\n",
            " |          a dict of column name and :class:`Column`. Currently, only a single map is supported.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with new or replaced columns.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df.withColumns({'age2': df.age + 2, 'age3': df.age + 3}).show()\n",
            " |      +---+-----+----+----+\n",
            " |      |age| name|age2|age3|\n",
            " |      +---+-----+----+----+\n",
            " |      |  2|Alice|   4|   5|\n",
            " |      |  5|  Bob|   7|   8|\n",
            " |      +---+-----+----+----+\n",
            " |  \n",
            " |  withColumnsRenamed(self, colsMap: Dict[str, str]) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` by renaming multiple columns.\n",
            " |      This is a no-op if the schema doesn't contain the given column names.\n",
            " |      \n",
            " |      .. versionadded:: 3.4.0\n",
            " |         Added support for multiple columns renaming\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      colsMap : dict\n",
            " |          a dict of existing column names and corresponding desired column names.\n",
            " |          Currently, only a single map is supported.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with renamed columns.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`withColumnRenamed`\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      Support Spark Connect\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df = df.withColumns({'age2': df.age + 2, 'age3': df.age + 3})\n",
            " |      >>> df.withColumnsRenamed({'age2': 'age4', 'age3': 'age5'}).show()\n",
            " |      +---+-----+----+----+\n",
            " |      |age| name|age4|age5|\n",
            " |      +---+-----+----+----+\n",
            " |      |  2|Alice|   4|   5|\n",
            " |      |  5|  Bob|   7|   8|\n",
            " |      +---+-----+----+----+\n",
            " |  \n",
            " |  withMetadata(self, columnName: str, metadata: Dict[str, Any]) -> 'DataFrame'\n",
            " |      Returns a new :class:`DataFrame` by updating an existing column with metadata.\n",
            " |      \n",
            " |      .. versionadded:: 3.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      columnName : str\n",
            " |          string, name of the existing column to update the metadata.\n",
            " |      metadata : dict\n",
            " |          dict, new metadata to be assigned to df.schema[columnName].metadata\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          DataFrame with updated metadata column.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> df_meta = df.withMetadata('age', {'foo': 'bar'})\n",
            " |      >>> df_meta.schema['age'].metadata\n",
            " |      {'foo': 'bar'}\n",
            " |  \n",
            " |  withWatermark(self, eventTime: str, delayThreshold: str) -> 'DataFrame'\n",
            " |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
            " |      in time before which we assume no more late data is going to arrive.\n",
            " |      \n",
            " |      Spark will use this watermark for several purposes:\n",
            " |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
            " |          when using output modes that do not allow updates.\n",
            " |      \n",
            " |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
            " |      \n",
            " |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
            " |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
            " |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
            " |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
            " |      process records that arrive more than `delayThreshold` late.\n",
            " |      \n",
            " |      .. versionadded:: 2.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.5.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      eventTime : str\n",
            " |          the name of the column that contains the event time of the row.\n",
            " |      delayThreshold : str\n",
            " |          the minimum delay to wait to data to arrive late, relative to the\n",
            " |          latest record that has been processed in the form of an interval\n",
            " |          (e.g. \"1 minute\" or \"5 hours\").\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrame`\n",
            " |          Watermarked DataFrame\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This is a feature only for Structured Streaming.\n",
            " |      \n",
            " |      This API is evolving.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql import Row\n",
            " |      >>> from pyspark.sql.functions import timestamp_seconds\n",
            " |      >>> df = spark.readStream.format(\"rate\").load().selectExpr(\n",
            " |      ...     \"value % 5 AS value\", \"timestamp\")\n",
            " |      >>> df.select(\"value\", df.timestamp.alias(\"time\")).withWatermark(\"time\", '10 minutes')\n",
            " |      DataFrame[value: bigint, time: timestamp]\n",
            " |      \n",
            " |      Group the data by window and value (0 - 4), and compute the count of each group.\n",
            " |      \n",
            " |      >>> import time\n",
            " |      >>> from pyspark.sql.functions import window\n",
            " |      >>> query = (df\n",
            " |      ...     .withWatermark(\"timestamp\", \"10 minutes\")\n",
            " |      ...     .groupBy(\n",
            " |      ...         window(df.timestamp, \"10 minutes\", \"5 minutes\"),\n",
            " |      ...         df.value)\n",
            " |      ...     ).count().writeStream.outputMode(\"complete\").format(\"console\").start()\n",
            " |      >>> time.sleep(3)\n",
            " |      >>> query.stop()\n",
            " |  \n",
            " |  writeTo(self, table: str) -> pyspark.sql.readwriter.DataFrameWriterV2\n",
            " |      Create a write configuration builder for v2 sources.\n",
            " |      \n",
            " |      This builder is used to configure and execute write operations.\n",
            " |      \n",
            " |      For example, to append or create or replace existing tables.\n",
            " |      \n",
            " |      .. versionadded:: 3.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      table : str\n",
            " |          Target table name to write to.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrameWriterV2`\n",
            " |          DataFrameWriterV2 to use further to specify how to save the data\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n",
            " |      >>> df.writeTo(                              # doctest: +SKIP\n",
            " |      ...     \"catalog.db.table\"\n",
            " |      ... ).partitionedBy(\"col\").createOrReplace()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  columns\n",
            " |      Retrieves the names of all columns in the :class:`DataFrame` as a list.\n",
            " |      \n",
            " |      The order of the column names in the list reflects their order in the DataFrame.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          List of column names in the DataFrame.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      Example 1: Retrieve column names of a DataFrame\n",
            " |      \n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\", \"CA\"), (23, \"Alice\", \"NY\"), (16, \"Bob\", \"TX\")],\n",
            " |      ...     [\"age\", \"name\", \"state\"]\n",
            " |      ... )\n",
            " |      >>> df.columns\n",
            " |      ['age', 'name', 'state']\n",
            " |      \n",
            " |      Example 2: Using column names to project specific columns\n",
            " |      \n",
            " |      >>> selected_cols = [col for col in df.columns if col != \"age\"]\n",
            " |      >>> df.select(selected_cols).show()\n",
            " |      +-----+-----+\n",
            " |      | name|state|\n",
            " |      +-----+-----+\n",
            " |      |  Tom|   CA|\n",
            " |      |Alice|   NY|\n",
            " |      |  Bob|   TX|\n",
            " |      +-----+-----+\n",
            " |      \n",
            " |      Example 3: Checking if a specific column exists in a DataFrame\n",
            " |      \n",
            " |      >>> \"state\" in df.columns\n",
            " |      True\n",
            " |      >>> \"salary\" in df.columns\n",
            " |      False\n",
            " |      \n",
            " |      Example 4: Iterating over columns to apply a transformation\n",
            " |      \n",
            " |      >>> import pyspark.sql.functions as f\n",
            " |      >>> for col_name in df.columns:\n",
            " |      ...     df = df.withColumn(col_name, f.upper(f.col(col_name)))\n",
            " |      >>> df.show()\n",
            " |      +---+-----+-----+\n",
            " |      |age| name|state|\n",
            " |      +---+-----+-----+\n",
            " |      | 14|  TOM|   CA|\n",
            " |      | 23|ALICE|   NY|\n",
            " |      | 16|  BOB|   TX|\n",
            " |      +---+-----+-----+\n",
            " |      \n",
            " |      Example 5: Renaming columns and checking the updated column names\n",
            " |      \n",
            " |      >>> df = df.withColumnRenamed(\"name\", \"first_name\")\n",
            " |      >>> df.columns\n",
            " |      ['age', 'first_name', 'state']\n",
            " |      \n",
            " |      Example 6: Using the `columns` property to ensure two DataFrames have the\n",
            " |      same columns before a union\n",
            " |      \n",
            " |      >>> df2 = spark.createDataFrame(\n",
            " |      ...     [(30, \"Eve\", \"FL\"), (40, \"Sam\", \"WA\")], [\"age\", \"name\", \"location\"])\n",
            " |      >>> df.columns == df2.columns\n",
            " |      False\n",
            " |  \n",
            " |  dtypes\n",
            " |      Returns all column names and their data types as a list.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          List of columns as tuple pairs.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      >>> df.dtypes\n",
            " |      [('age', 'bigint'), ('name', 'string')]\n",
            " |  \n",
            " |  isStreaming\n",
            " |      Returns ``True`` if this :class:`DataFrame` contains one or more sources that\n",
            " |      continuously return data as it arrives. A :class:`DataFrame` that reads data from a\n",
            " |      streaming source must be executed as a :class:`StreamingQuery` using the :func:`start`\n",
            " |      method in :class:`DataStreamWriter`.  Methods that return a single answer, (e.g.,\n",
            " |      :func:`count` or :func:`collect`) will throw an :class:`AnalysisException` when there\n",
            " |      is a streaming source present.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is evolving.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      bool\n",
            " |          Whether it's streaming DataFrame or not.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.readStream.format(\"rate\").load()\n",
            " |      >>> df.isStreaming\n",
            " |      True\n",
            " |  \n",
            " |  na\n",
            " |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.1\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrameNaFunctions`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.sql(\"SELECT 1 AS c1, int(NULL) AS c2\")\n",
            " |      >>> type(df.na)\n",
            " |      <class '...dataframe.DataFrameNaFunctions'>\n",
            " |      \n",
            " |      Replace the missing values as 2.\n",
            " |      \n",
            " |      >>> df.na.fill(2).show()\n",
            " |      +---+---+\n",
            " |      | c1| c2|\n",
            " |      +---+---+\n",
            " |      |  1|  2|\n",
            " |      +---+---+\n",
            " |  \n",
            " |  rdd\n",
            " |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDD`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(1)\n",
            " |      >>> type(df.rdd)\n",
            " |      <class 'pyspark.rdd.RDD'>\n",
            " |  \n",
            " |  schema\n",
            " |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`StructType`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            " |      \n",
            " |      Retrieve the schema of the current DataFrame.\n",
            " |      \n",
            " |      >>> df.schema\n",
            " |      StructType([StructField('age', LongType(), True),\n",
            " |                  StructField('name', StringType(), True)])\n",
            " |  \n",
            " |  sparkSession\n",
            " |      Returns Spark session that created this :class:`DataFrame`.\n",
            " |      \n",
            " |      .. versionadded:: 3.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`SparkSession`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.range(1)\n",
            " |      >>> type(df.sparkSession)\n",
            " |      <class '...session.SparkSession'>\n",
            " |  \n",
            " |  sql_ctx\n",
            " |  \n",
            " |  stat\n",
            " |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrameStatFunctions`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import pyspark.sql.functions as f\n",
            " |      >>> df = spark.range(3).withColumn(\"c\", f.expr(\"id + 1\"))\n",
            " |      >>> type(df.stat)\n",
            " |      <class '...dataframe.DataFrameStatFunctions'>\n",
            " |      >>> df.stat.corr(\"id\", \"c\")\n",
            " |      1.0\n",
            " |  \n",
            " |  storageLevel\n",
            " |      Get the :class:`DataFrame`'s current storage level.\n",
            " |      \n",
            " |      .. versionadded:: 2.1.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`StorageLevel`\n",
            " |          Currently defined storage level.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df1 = spark.range(10)\n",
            " |      >>> df1.storageLevel\n",
            " |      StorageLevel(False, False, False, False, 1)\n",
            " |      >>> df1.cache().storageLevel\n",
            " |      StorageLevel(True, True, False, True, 1)\n",
            " |      \n",
            " |      >>> df2 = spark.range(5)\n",
            " |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
            " |      StorageLevel(True, False, False, False, 2)\n",
            " |  \n",
            " |  write\n",
            " |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
            " |      storage.\n",
            " |      \n",
            " |      .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataFrameWriter`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            " |      >>> type(df.write)\n",
            " |      <class '...readwriter.DataFrameWriter'>\n",
            " |      \n",
            " |      Write the DataFrame as a table.\n",
            " |      \n",
            " |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tab2\")\n",
            " |      >>> df.write.saveAsTable(\"tab2\")\n",
            " |      >>> _ = spark.sql(\"DROP TABLE tab2\")\n",
            " |  \n",
            " |  writeStream\n",
            " |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
            " |      storage.\n",
            " |      \n",
            " |      .. versionadded:: 2.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.5.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is evolving.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`DataStreamWriter`\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import tempfile\n",
            " |      >>> df = spark.readStream.format(\"rate\").load()\n",
            " |      >>> type(df.writeStream)\n",
            " |      <class '...streaming.readwriter.DataStreamWriter'>\n",
            " |      \n",
            " |      >>> with tempfile.TemporaryDirectory() as d:\n",
            " |      ...     # Create a table with Rate source.\n",
            " |      ...     df.writeStream.toTable(\n",
            " |      ...         \"my_table\", checkpointLocation=d)\n",
            " |      <...streaming.query.StreamingQuery object at 0x...>\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
            " |  \n",
            " |  mapInArrow(self, func: 'ArrowMapIterFunction', schema: Union[pyspark.sql.types.StructType, str], barrier: bool = False) -> 'DataFrame'\n",
            " |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
            " |      function that takes and outputs a PyArrow's `RecordBatch`, and returns the result as a\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      The function should take an iterator of `pyarrow.RecordBatch`\\s and return\n",
            " |      another iterator of `pyarrow.RecordBatch`\\s. All columns are passed\n",
            " |      together as an iterator of `pyarrow.RecordBatch`\\s to the function and the\n",
            " |      returned iterator of `pyarrow.RecordBatch`\\s are combined as a :class:`DataFrame`.\n",
            " |      Each `pyarrow.RecordBatch` size can be controlled by\n",
            " |      `spark.sql.execution.arrow.maxRecordsPerBatch`. The size of the function's input and\n",
            " |      output can be different.\n",
            " |      \n",
            " |      .. versionadded:: 3.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      func : function\n",
            " |          a Python native function that takes an iterator of `pyarrow.RecordBatch`\\s, and\n",
            " |          outputs an iterator of `pyarrow.RecordBatch`\\s.\n",
            " |      schema : :class:`pyspark.sql.types.DataType` or str\n",
            " |          the return type of the `func` in PySpark. The value can be either a\n",
            " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
            " |      barrier : bool, optional, default False\n",
            " |          Use barrier mode execution.\n",
            " |      \n",
            " |          .. versionadded: 3.5.0\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> import pyarrow  # doctest: +SKIP\n",
            " |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
            " |      >>> def filter_func(iterator):\n",
            " |      ...     for batch in iterator:\n",
            " |      ...         pdf = batch.to_pandas()\n",
            " |      ...         yield pyarrow.RecordBatch.from_pandas(pdf[pdf.id == 1])\n",
            " |      >>> df.mapInArrow(filter_func, df.schema).show()  # doctest: +SKIP\n",
            " |      +---+---+\n",
            " |      | id|age|\n",
            " |      +---+---+\n",
            " |      |  1| 21|\n",
            " |      +---+---+\n",
            " |      \n",
            " |      Set ``barrier`` to ``True`` to force the ``mapInArrow`` stage running in the\n",
            " |      barrier mode, it ensures all Python workers in the stage will be\n",
            " |      launched concurrently.\n",
            " |      \n",
            " |      >>> df.mapInArrow(filter_func, df.schema, barrier=True).show()  # doctest: +SKIP\n",
            " |      +---+---+\n",
            " |      | id|age|\n",
            " |      +---+---+\n",
            " |      |  1| 21|\n",
            " |      +---+---+\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is unstable, and for developers.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      pyspark.sql.functions.pandas_udf\n",
            " |      pyspark.sql.DataFrame.mapInPandas\n",
            " |  \n",
            " |  mapInPandas(self, func: 'PandasMapIterFunction', schema: Union[pyspark.sql.types.StructType, str], barrier: bool = False) -> 'DataFrame'\n",
            " |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
            " |      function that takes and outputs a pandas DataFrame, and returns the result as a\n",
            " |      :class:`DataFrame`.\n",
            " |      \n",
            " |      The function should take an iterator of `pandas.DataFrame`\\s and return\n",
            " |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n",
            " |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n",
            " |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n",
            " |      Each `pandas.DataFrame` size can be controlled by\n",
            " |      `spark.sql.execution.arrow.maxRecordsPerBatch`. The size of the function's input and\n",
            " |      output can be different.\n",
            " |      \n",
            " |      .. versionadded:: 3.0.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      func : function\n",
            " |          a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n",
            " |          outputs an iterator of `pandas.DataFrame`\\s.\n",
            " |      schema : :class:`pyspark.sql.types.DataType` or str\n",
            " |          the return type of the `func` in PySpark. The value can be either a\n",
            " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
            " |      barrier : bool, optional, default False\n",
            " |          Use barrier mode execution.\n",
            " |      \n",
            " |          .. versionadded: 3.5.0\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from pyspark.sql.functions import pandas_udf\n",
            " |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
            " |      >>> def filter_func(iterator):\n",
            " |      ...     for pdf in iterator:\n",
            " |      ...         yield pdf[pdf.id == 1]\n",
            " |      ...\n",
            " |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
            " |      +---+---+\n",
            " |      | id|age|\n",
            " |      +---+---+\n",
            " |      |  1| 21|\n",
            " |      +---+---+\n",
            " |      \n",
            " |      Set ``barrier`` to ``True`` to force the ``mapInPandas`` stage running in the\n",
            " |      barrier mode, it ensures all Python workers in the stage will be\n",
            " |      launched concurrently.\n",
            " |      \n",
            " |      >>> df.mapInPandas(filter_func, df.schema, barrier=True).show()  # doctest: +SKIP\n",
            " |      +---+---+\n",
            " |      | id|age|\n",
            " |      +---+---+\n",
            " |      |  1| 21|\n",
            " |      +---+---+\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is experimental\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      pyspark.sql.functions.pandas_udf\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n",
            " |  \n",
            " |  toPandas(self) -> 'PandasDataFrameLike'\n",
            " |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
            " |      \n",
            " |      This is only available if Pandas is installed and available.\n",
            " |      \n",
            " |      .. versionadded:: 1.3.0\n",
            " |      \n",
            " |      .. versionchanged:: 3.4.0\n",
            " |          Supports Spark Connect.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting Pandas ``pandas.DataFrame`` is\n",
            " |      expected to be small, as all the data is loaded into the driver's memory.\n",
            " |      \n",
            " |      Usage with ``spark.sql.execution.arrow.pyspark.enabled=True`` is experimental.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> df.toPandas()  # doctest: +SKIP\n",
            " |         age   name\n",
            " |      0    2  Alice\n",
            " |      1    5    Bob\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(book)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8718X690mWsd",
        "outputId": "6f9e0abf-c289-40c8-ba89-13ebd23b662d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_collect_as_arrow',\n",
              " '_ipython_key_completions_',\n",
              " '_jcols',\n",
              " '_jdf',\n",
              " '_jmap',\n",
              " '_joinAsOf',\n",
              " '_jseq',\n",
              " '_lazy_rdd',\n",
              " '_repr_html_',\n",
              " '_sc',\n",
              " '_schema',\n",
              " '_session',\n",
              " '_show_string',\n",
              " '_sort_cols',\n",
              " '_sql_ctx',\n",
              " '_support_repr_html',\n",
              " 'agg',\n",
              " 'alias',\n",
              " 'approxQuantile',\n",
              " 'cache',\n",
              " 'checkpoint',\n",
              " 'coalesce',\n",
              " 'colRegex',\n",
              " 'collect',\n",
              " 'columns',\n",
              " 'corr',\n",
              " 'count',\n",
              " 'cov',\n",
              " 'createGlobalTempView',\n",
              " 'createOrReplaceGlobalTempView',\n",
              " 'createOrReplaceTempView',\n",
              " 'createTempView',\n",
              " 'crossJoin',\n",
              " 'crosstab',\n",
              " 'cube',\n",
              " 'describe',\n",
              " 'distinct',\n",
              " 'drop',\n",
              " 'dropDuplicates',\n",
              " 'dropDuplicatesWithinWatermark',\n",
              " 'drop_duplicates',\n",
              " 'dropna',\n",
              " 'dtypes',\n",
              " 'exceptAll',\n",
              " 'explain',\n",
              " 'fillna',\n",
              " 'filter',\n",
              " 'first',\n",
              " 'foreach',\n",
              " 'foreachPartition',\n",
              " 'freqItems',\n",
              " 'groupBy',\n",
              " 'groupby',\n",
              " 'head',\n",
              " 'hint',\n",
              " 'inputFiles',\n",
              " 'intersect',\n",
              " 'intersectAll',\n",
              " 'isEmpty',\n",
              " 'isLocal',\n",
              " 'isStreaming',\n",
              " 'is_cached',\n",
              " 'join',\n",
              " 'limit',\n",
              " 'localCheckpoint',\n",
              " 'mapInArrow',\n",
              " 'mapInPandas',\n",
              " 'melt',\n",
              " 'na',\n",
              " 'observe',\n",
              " 'offset',\n",
              " 'orderBy',\n",
              " 'pandas_api',\n",
              " 'persist',\n",
              " 'printSchema',\n",
              " 'randomSplit',\n",
              " 'rdd',\n",
              " 'registerTempTable',\n",
              " 'repartition',\n",
              " 'repartitionByRange',\n",
              " 'replace',\n",
              " 'rollup',\n",
              " 'sameSemantics',\n",
              " 'sample',\n",
              " 'sampleBy',\n",
              " 'schema',\n",
              " 'select',\n",
              " 'selectExpr',\n",
              " 'semanticHash',\n",
              " 'show',\n",
              " 'sort',\n",
              " 'sortWithinPartitions',\n",
              " 'sparkSession',\n",
              " 'sql_ctx',\n",
              " 'stat',\n",
              " 'storageLevel',\n",
              " 'subtract',\n",
              " 'summary',\n",
              " 'tail',\n",
              " 'take',\n",
              " 'to',\n",
              " 'toDF',\n",
              " 'toJSON',\n",
              " 'toLocalIterator',\n",
              " 'toPandas',\n",
              " 'to_koalas',\n",
              " 'to_pandas_on_spark',\n",
              " 'transform',\n",
              " 'union',\n",
              " 'unionAll',\n",
              " 'unionByName',\n",
              " 'unpersist',\n",
              " 'unpivot',\n",
              " 'value',\n",
              " 'where',\n",
              " 'withColumn',\n",
              " 'withColumnRenamed',\n",
              " 'withColumns',\n",
              " 'withColumnsRenamed',\n",
              " 'withMetadata',\n",
              " 'withWatermark',\n",
              " 'write',\n",
              " 'writeStream',\n",
              " 'writeTo']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(book.printSchema)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGwEMIX7nDr3",
        "outputId": "3d046867-a3a8-43df-aff3-2f45861f52c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method printSchema in module pyspark.sql.dataframe:\n",
            "\n",
            "printSchema(level: Optional[int] = None) -> None method of pyspark.sql.dataframe.DataFrame instance\n",
            "    Prints out the schema in the tree format.\n",
            "    Optionally allows to specify how many levels to print if schema is nested.\n",
            "    \n",
            "    .. versionadded:: 1.3.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    level : int, optional, default None\n",
            "        How many levels to print for nested schemas.\n",
            "    \n",
            "        .. versionchanged:: 3.5.0\n",
            "            Added Level parameter.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame(\n",
            "    ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "    >>> df.printSchema()\n",
            "    root\n",
            "     |-- age: long (nullable = true)\n",
            "     |-- name: string (nullable = true)\n",
            "    \n",
            "    >>> df = spark.createDataFrame([(1, (2,2))], [\"a\", \"b\"])\n",
            "    >>> df.printSchema(1)\n",
            "    root\n",
            "     |-- a: long (nullable = true)\n",
            "     |-- b: struct (nullable = true)\n",
            "    \n",
            "    >>> df.printSchema(2)\n",
            "    root\n",
            "     |-- a: long (nullable = true)\n",
            "     |-- b: struct (nullable = true)\n",
            "     |    |-- _1: long (nullable = true)\n",
            "     |    |-- _2: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "book.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZWZXVZZSJZO",
        "outputId": "f69103b4-c6c2-4103-df06-78c38ea53c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- value: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#results.printSchema()"
      ],
      "metadata": {
        "id": "44HhGV9nTmc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#book.show()"
      ],
      "metadata": {
        "id": "hMsiBPCfS-qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#results.show()"
      ],
      "metadata": {
        "id": "nTnctUZWTepp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(book.show)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrey6z8pnlIN",
        "outputId": "5dd241ee-c1e8-4afc-d6fe-41871e52814a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method show in module pyspark.sql.dataframe:\n",
            "\n",
            "show(n: int = 20, truncate: Union[bool, int] = True, vertical: bool = False) -> None method of pyspark.sql.dataframe.DataFrame instance\n",
            "    Prints the first ``n`` rows to the console.\n",
            "    \n",
            "    .. versionadded:: 1.3.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    n : int, optional\n",
            "        Number of rows to show.\n",
            "    truncate : bool or int, optional\n",
            "        If set to ``True``, truncate strings longer than 20 chars by default.\n",
            "        If set to a number greater than one, truncates long strings to length ``truncate``\n",
            "        and align cells right.\n",
            "    vertical : bool, optional\n",
            "        If set to ``True``, print output rows vertically (one line\n",
            "        per column value).\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame([\n",
            "    ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
            "    \n",
            "    Show only top 2 rows.\n",
            "    \n",
            "    >>> df.show(2)\n",
            "    +---+-----+\n",
            "    |age| name|\n",
            "    +---+-----+\n",
            "    | 14|  Tom|\n",
            "    | 23|Alice|\n",
            "    +---+-----+\n",
            "    only showing top 2 rows\n",
            "    \n",
            "    Show :class:`DataFrame` where the maximum number of characters is 3.\n",
            "    \n",
            "    >>> df.show(truncate=3)\n",
            "    +---+----+\n",
            "    |age|name|\n",
            "    +---+----+\n",
            "    | 14| Tom|\n",
            "    | 23| Ali|\n",
            "    | 16| Bob|\n",
            "    +---+----+\n",
            "    \n",
            "    Show :class:`DataFrame` vertically.\n",
            "    \n",
            "    >>> df.show(vertical=True)\n",
            "    -RECORD 0-----\n",
            "    age  | 14\n",
            "    name | Tom\n",
            "    -RECORD 1-----\n",
            "    age  | 23\n",
            "    name | Alice\n",
            "    -RECORD 2-----\n",
            "    age  | 16\n",
            "    name | Bob\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "book.show(5, truncate=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5jwpS-bTkY6",
        "outputId": "c7e874ef-efe1-4b57-ca18-c0cce237f2a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------+\n",
            "|                                             value|\n",
            "+--------------------------------------------------+\n",
            "|The Project Gutenberg EBook of Pride and Prejud...|\n",
            "|                                                  |\n",
            "|This eBook is for the use of anyone anywhere at...|\n",
            "|almost no restrictions whatsoever.  You may cop...|\n",
            "|re-use it under the terms of the Project Gutenb...|\n",
            "+--------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " #results.show(10, truncate=50)"
      ],
      "metadata": {
        "id": "13rJB_LWUWat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "L_rVSY2SoXFI",
        "outputId": "5c166dbc-5519-4dea-c290-d87d8f1e1759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "Invalid star expression (<ipython-input-29-141505132>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-141505132>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    book.select(*).show()\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Invalid star expression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(book.select)"
      ],
      "metadata": {
        "id": "NHkhhGdCWS_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92b7f58a-7353-4a8e-d384-8a9eb0a5a431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method select in module pyspark.sql.dataframe:\n",
            "\n",
            "select(*cols: 'ColumnOrName') -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
            "    Projects a set of expressions and returns a new :class:`DataFrame`.\n",
            "    \n",
            "    .. versionadded:: 1.3.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    cols : str, :class:`Column`, or list\n",
            "        column names (string) or expressions (:class:`Column`).\n",
            "        If one of the column names is '*', that column is expanded to include all columns\n",
            "        in the current :class:`DataFrame`.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    :class:`DataFrame`\n",
            "        A DataFrame with subset (or all) of columns.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = spark.createDataFrame([\n",
            "    ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
            "    \n",
            "    Select all columns in the DataFrame.\n",
            "    \n",
            "    >>> df.select('*').show()\n",
            "    +---+-----+\n",
            "    |age| name|\n",
            "    +---+-----+\n",
            "    |  2|Alice|\n",
            "    |  5|  Bob|\n",
            "    +---+-----+\n",
            "    \n",
            "    Select a column with other expressions in the DataFrame.\n",
            "    \n",
            "    >>> df.select(df.name, (df.age + 10).alias('age')).show()\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice| 12|\n",
            "    |  Bob| 15|\n",
            "    +-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "9Zk4LepJWThd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#help(book.select(split(book.value,\" \")))"
      ],
      "metadata": {
        "id": "Hj8niprrpJlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        " from pyspark.sql.functions import split\n",
        " lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
        " lines.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8QQNI4cUY_m",
        "outputId": "778a4b9f-b0f3-45d1-9f29-2088c599d268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                line|\n",
            "+--------------------+\n",
            "|[The, Project, Gu...|\n",
            "|                  []|\n",
            "|[This, eBook, is,...|\n",
            "|[almost, no, rest...|\n",
            "|[re-use, it, unde...|\n",
            "|[with, this, eBoo...|\n",
            "|                  []|\n",
            "|                  []|\n",
            "|[Title:, Pride, a...|\n",
            "|                  []|\n",
            "+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " book.select(book.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo8eJwTDVquf",
        "outputId": "464570e0-5173-4aac-8b4e-b9573aa83e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[value: string]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# different ways of selecting\n",
        "from pyspark.sql.functions import col\n",
        "book.select(book.value)\n",
        "book.select(book[\"value\"])\n",
        "book.select(col(\"value\"))\n",
        "book.select(\"value\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5k6cY5TWNQp",
        "outputId": "5ea552b1-2ae9-4298-dff3-a1bb529e9c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[value: string]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, split\n",
        "lines = book.select(split(col(\"value\"), \" \"))\n",
        "lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSe5_1nBW0dK",
        "outputId": "7923fd95-d76e-451e-eb73-543ef12d881b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[split(value,  , -1): array<string>]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-- split(value,  , -1)  this is a default name\n",
        " lines.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-9q5oc_Xpsd",
        "outputId": "03f030a2-700d-41e4-977b-6f63d40b1d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- split(value,  , -1): array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1KV6vHTXt_6",
        "outputId": "70632afe-e831-4ca8-9018-c44f649c81d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                line|\n",
            "+--------------------+\n",
            "|[The, Project, Gu...|\n",
            "|                  []|\n",
            "|[This, eBook, is,...|\n",
            "|[almost, no, rest...|\n",
            "|[re-use, it, unde...|\n",
            "+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "book.select(split(col(\"value\"), \" \")).printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMRgz2D5XwC0",
        "outputId": "a88915d7-ba5b-43b9-a2a1-6c3bfdd77b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- split(value,  , -1): array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " book.select(split(col(\"value\"), \" \").alias(\"line\")).printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y8odV3eYlMz",
        "outputId": "ee9174a0-a105-40e7-f0b1-7781dfe08bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- line: array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
        "lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLw6JCuDYns9",
        "outputId": "568f430f-47e0-4504-dc67-f86fafb70c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[line: array<string>]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # This is messier, and you have to remember the name PySpark assigns automatically\n",
        "lines = book.select(split(book.value, \" \"))\n",
        "lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmyFL-FnYoHK",
        "outputId": "7f1d57dd-46f2-4e8c-d337-827e0a64f2e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[split(value,  , -1): array<string>]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = lines.withColumnRenamed(\"split(value,  , -1)\", \"line\")\n",
        "lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmdIZOVMZU-L",
        "outputId": "ba5c8ce9-905a-4a41-bf0f-482d809595fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[line: array<string>]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col\n",
        "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
        "words.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgKJ8d8JZnCu",
        "outputId": "297893ac-5502-4527-b1b7-138a5d173660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|      word|\n",
            "+----------+\n",
            "|       The|\n",
            "|   Project|\n",
            "| Gutenberg|\n",
            "|     EBook|\n",
            "|        of|\n",
            "|     Pride|\n",
            "|       and|\n",
            "|Prejudice,|\n",
            "|        by|\n",
            "|      Jane|\n",
            "|    Austen|\n",
            "|          |\n",
            "|      This|\n",
            "|     eBook|\n",
            "|        is|\n",
            "|       for|\n",
            "|       the|\n",
            "|       use|\n",
            "|        of|\n",
            "|    anyone|\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lower\n",
        "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
        "words_lower"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9HDBZxTZqcO",
        "outputId": "cd11f42a-90e5-43f8-b394-1055837e394f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[word_lower: string]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_lower.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGvX0O0TZrWy",
        "outputId": "d4e42e45-575b-4083-e1c3-ae49de82e914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|word_lower|\n",
            "+----------+\n",
            "|       the|\n",
            "|   project|\n",
            "| gutenberg|\n",
            "|     ebook|\n",
            "|        of|\n",
            "|     pride|\n",
            "|       and|\n",
            "|prejudice,|\n",
            "|        by|\n",
            "|      jane|\n",
            "|    austen|\n",
            "|          |\n",
            "|      this|\n",
            "|     ebook|\n",
            "|        is|\n",
            "|       for|\n",
            "|       the|\n",
            "|       use|\n",
            "|        of|\n",
            "|    anyone|\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# import os\n",
        "# from pyspark.sql import SparkSession\n",
        "# from pyspark.sql import functions as F\n",
        "\n",
        "# # Mount Google Drive (ensure it's mounted before running)\n",
        "# drive_path = \"/content/drive/MyDrive/cde_data\"  # Change this to your preferred folder in Google Drive\n",
        "# os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# # GitHub raw file URL\n",
        "# github_url = \"https://github.com/kareemullah123456789/DataAnalysisWithPythonAndPySpark-Data/raw/refs/heads/trunk/list_of_numbers/sample.csv\"\n",
        "\n",
        "# # Save file in Google Drive\n",
        "# numbers_filename = os.path.join(drive_path, \"sample.csv\")\n",
        "\n",
        "# # Download the file from GitHub\n",
        "# response = requests.get(github_url)\n",
        "# with open(numbers_filename, \"wb\") as f:\n",
        "#     f.write(response.content)\n",
        "\n",
        "# print(f\"File downloaded to: {local_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR8UaVPCZrly",
        "outputId": "d107469a-c390-4678-fbb5-df84db22d200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded to: /content/drive/MyDrive/cde_data/1342-0.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numbers_filename"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UagkDEdLZr0d",
        "outputId": "07bf6ac3-16aa-4b37-da5a-10393cd7767e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/cde_data/sample.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number_df = spark.read.text(numbers_filename)"
      ],
      "metadata": {
        "id": "7GRsbAbGbjHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5-3D3SObxj2",
        "outputId": "f6b11912-5fe3-4fdf-adb0-3fa3dac59524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|     value|\n",
            "+----------+\n",
            "|old_column|\n",
            "|         1|\n",
            "|         4|\n",
            "|         4|\n",
            "|         5|\n",
            "|         7|\n",
            "|         7|\n",
            "|         7|\n",
            "|        10|\n",
            "|        14|\n",
            "|         1|\n",
            "|         4|\n",
            "|         8|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql.functions import explode, split, col\n",
        "\n",
        "# solution_2_1_df = number_df.select(explode(split(col(\"value\"), \",\")).alias(\"exploded_value\"))\n",
        "# # solution_2_1_df.show()"
      ],
      "metadata": {
        "id": "7kRGN-yEb3DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_extract\n",
        "words_clean = words_lower.select(\n",
        "    regexp_extract(col(\"word_lower\"), \"[a-z]+\", 0).alias(\"word\")\n",
        ")\n",
        "words_clean.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjNn0haocHpl",
        "outputId": "d52e1db5-4e5d-42e2-b0e4-6ec51a06b63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|     word|\n",
            "+---------+\n",
            "|      the|\n",
            "|  project|\n",
            "|gutenberg|\n",
            "|    ebook|\n",
            "|       of|\n",
            "|    pride|\n",
            "|      and|\n",
            "|prejudice|\n",
            "|       by|\n",
            "|     jane|\n",
            "|   austen|\n",
            "|         |\n",
            "|     this|\n",
            "|    ebook|\n",
            "|       is|\n",
            "|      for|\n",
            "|      the|\n",
            "|      use|\n",
            "|       of|\n",
            "|   anyone|\n",
            "+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_nonull = words_clean.filter(col(\"word\") != \"\")\n",
        "words_nonull.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfdlV9gIc_z7",
        "outputId": "4da35cd6-5c03-4608-8e73-23906c376d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|     word|\n",
            "+---------+\n",
            "|      the|\n",
            "|  project|\n",
            "|gutenberg|\n",
            "|    ebook|\n",
            "|       of|\n",
            "|    pride|\n",
            "|      and|\n",
            "|prejudice|\n",
            "|       by|\n",
            "|     jane|\n",
            "|   austen|\n",
            "|     this|\n",
            "|    ebook|\n",
            "|       is|\n",
            "|      for|\n",
            "|      the|\n",
            "|      use|\n",
            "|       of|\n",
            "|   anyone|\n",
            "| anywhere|\n",
            "+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# groups = words_nonull.groupby(col(\"word\"))\n",
        "# print(groups)\n",
        " # <pyspark.sql.group.GroupedData at 0x10ed23da0>\n",
        "results = words_nonull.groupby(col(\"word\")).count()\n",
        "print(results)\n",
        " # DataFrame[word: string, count: bigint]\n",
        "results.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDE-dZz2dAHE",
        "outputId": "7d0e6a83-a7b8-4f86-b05a-b144163d3188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[word: string, count: bigint]\n",
            "+-------------+-----+\n",
            "|         word|count|\n",
            "+-------------+-----+\n",
            "|       online|    4|\n",
            "|         some|  209|\n",
            "|        still|   72|\n",
            "|          few|   72|\n",
            "|         hope|  122|\n",
            "|        those|   60|\n",
            "|     cautious|    4|\n",
            "|    imitation|    1|\n",
            "|          art|    3|\n",
            "|      solaced|    1|\n",
            "|       poetry|    2|\n",
            "|    arguments|    5|\n",
            "| premeditated|    1|\n",
            "|      elevate|    1|\n",
            "|       doubts|    2|\n",
            "|    destitute|    1|\n",
            "|    solemnity|    5|\n",
            "|   lieutenant|    1|\n",
            "|gratification|    1|\n",
            "|    connected|   14|\n",
            "+-------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.orderBy(\"count\", ascending=True).show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp00-E-ceJ4L",
        "outputId": "2692d3c2-1390-470f-d8ce-ba5bc98c0396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|         word|count|\n",
            "+-------------+-----+\n",
            "|       coaxed|    1|\n",
            "|   lieutenant|    1|\n",
            "|        pools|    1|\n",
            "| premeditated|    1|\n",
            "|intermarriage|    1|\n",
            "|      solaced|    1|\n",
            "|   reanimated|    1|\n",
            "|      elevate|    1|\n",
            "|     singling|    1|\n",
            "|  irreligious|    1|\n",
            "+-------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.orderBy(col(\"count\").desc()).show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzJ9Aq4XeKNk",
        "outputId": "78dcd592-a4ad-40f6-c234-c8cea9b3a041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "| the| 4496|\n",
            "|  to| 4235|\n",
            "|  of| 3719|\n",
            "| and| 3602|\n",
            "| her| 2223|\n",
            "|   i| 2052|\n",
            "|   a| 1997|\n",
            "|  in| 1920|\n",
            "| was| 1844|\n",
            "| she| 1703|\n",
            "+----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " results.write.csv(\"/content/drive/MyDrive/cde_data/simple_count_words.csv\")\n",
        " # The ls command is run using a shell, not a Python prompt.\n",
        " # If you use IPython, you can use the bang pattern (! ls -1).\n",
        " # Use this to get the same results without leaving the IPython console.\n"
      ],
      "metadata": {
        "id": "yvuLnplDe53D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.repartition(4).write.csv(\"/content/drive/MyDrive/cde_data/simple_count_words_4_partitions.csv\")"
      ],
      "metadata": {
        "id": "5r0Wppr8y3d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coalesce\n",
        "df= spark.read.csv(\"/content/drive/MyDrive/cde_data/simple_count_words_4_partitions.csv\",header=True)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfPHzfBL0Jy5",
        "outputId": "728090e4-88c1-4dbf-caff-5fff705ebe8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+---+\n",
            "|     frailty|  1|\n",
            "+------------+---+\n",
            "|disbelieving|  1|\n",
            "|       abuse|  2|\n",
            "|       lying|  1|\n",
            "|        held|  5|\n",
            "|   vulgarity|  1|\n",
            "|    hastened|  4|\n",
            "|  lieutenant|  1|\n",
            "|    strength|  6|\n",
            "|    escaping|  1|\n",
            "| presumption|  3|\n",
            "|      obtain|  6|\n",
            "|      canvas|  1|\n",
            "|        amid|  1|\n",
            "|      rattle|  2|\n",
            "|   retaliate|  1|\n",
            "|   despaired|  3|\n",
            "|   depending|  2|\n",
            "|    beauties|  4|\n",
            "|    exercise|  6|\n",
            "|   separates|  1|\n",
            "+------------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.coalesce(1).write.csv(\"/content/drive/MyDrive/cde_data/simple_6_partitions.csv\")"
      ],
      "metadata": {
        "id": "x6hzElzp1HDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GGkB_HK81TKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wRrtwCtc1RDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LwXWT-Wh1QNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#results.write.partitionBy(\"word\").csv(\"/content/drive/MyDrive/cde_data/simple_count_words_partitioned1.csv\",header=True)"
      ],
      "metadata": {
        "id": "XE5_x4oIzUl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls -1 /content/drive/MyDrive/cde_data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB1vH-9wyevQ",
        "outputId": "e4840c58-819c-48e5-ffd0-41e81f1b4037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1342-0.txt\n",
            "\u001b[0m\u001b[01;34mbroadcast_logs\u001b[0m/\n",
            "data.csv\n",
            "data_Q3_2019.zip\n",
            "\u001b[01;34melements\u001b[0m/\n",
            "\u001b[01;34mgsod_noaa\u001b[0m/\n",
            "\u001b[01;34mgutenberg_books\u001b[0m/\n",
            "\u001b[01;34mprideandprejudice.csv\u001b[0m/\n",
            "'pyngrok_UI_CODE_Working_with_RDDs_in_PySpark_(2).ipynb'\n",
            "pyspark_tutorial.ipynb\n",
            "sales_data.csv\n",
            "sample.csv\n",
            "sample_data.csv\n",
            "\u001b[01;34msample_section_4\u001b[0m/\n",
            "Section_2_Resilient_Distributed_Datasets_Transformations.ipynb\n",
            "Section_3_Resilient_Distributed_Datasets_Actions.ipynb\n",
            "Section_4_Spark_DataFrames_and_Transformations.ipynb\n",
            "\u001b[01;34mshows\u001b[0m/\n",
            "\u001b[01;34msimple_count2.csv\u001b[0m/\n",
            "\u001b[01;34msimple_count.csv\u001b[0m/\n",
            "\u001b[01;34msimple_count_single_partition.csv\u001b[0m/\n",
            "\u001b[01;34msimple_count_single_partition_final.csv\u001b[0m/\n",
            "\u001b[01;34msimple_count_words.csv\u001b[0m/\n",
            "Spark_SQL.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ls -1 /content/drive/MyDrive/cde_data/simple_count.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoCioa6zfkJK",
        "outputId": "164fccce-c50a-4404-f558-29b61ab255f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-53ef1dc5-b4c7-4720-8e4f-283551babb3b-c000.csv\n",
            "_SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " results.coalesce(1).write.csv(\"/content/drive/MyDrive/cde_data/simple_count_single_partition.csv\")"
      ],
      "metadata": {
        "id": "-vHljxrdfyAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NCKSyBiRgy5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from pyspark.sql import SparkSession\n",
        " from pyspark.sql.functions import (\n",
        "    col,\n",
        "    explode,\n",
        "    lower,\n",
        "    regexp_extract,\n",
        "    split,\n",
        " )\n",
        " spark = SparkSession.builder.appName(\n",
        "    \"Analyzing the vocabulary of Pride and Prejudice.\"\n",
        " ).getOrCreate()\n",
        "book_final = spark.read.text(\"/content/drive/MyDrive/cde_data/1342-0.txt\")\n",
        "lines_final = book_final.select(split(book_final.value, \" \").alias(\"line\"))\n",
        "words_final = lines_final.select(explode(col(\"line\")).alias(\"word\"))\n",
        "words_lower_final = words_final.select(lower(col(\"word\")).alias(\"word\"))\n",
        "words_clean_final = words_lower_final.select(\n",
        "    regexp_extract(col(\"word\"), \"[a-z']*\", 0).alias(\"word\")\n",
        " )\n",
        "words_nonull_final = words_clean_final.where(col(\"word\") != \"\")\n",
        "results_final = words_nonull_final.groupby(col(\"word\")).count()\n",
        "results_final.orderBy(\"count\", ascending=False).show(10)\n",
        "results_final.coalesce(1).write.csv(\"/content/drive/MyDrive/cde_data//simple_count_single_partition_final.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl8z3xEDgzRG",
        "outputId": "275ea687-e580-415a-ee01-296d9784bed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "| the| 4480|\n",
            "|  to| 4218|\n",
            "|  of| 3711|\n",
            "| and| 3504|\n",
            "| her| 2199|\n",
            "|   a| 1982|\n",
            "|  in| 1909|\n",
            "| was| 1838|\n",
            "|   i| 1749|\n",
            "| she| 1668|\n",
            "+----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Simplifying our PySpark functions import\n",
        " # Before\n",
        " from pyspark.sql.functions import col, explode, lower, regexp_extract, split\n",
        " # After\n",
        " import pyspark.sql.functions as F"
      ],
      "metadata": {
        "id": "jo1GF7Xjhjc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #  Simplifying our program via method chaining\n",
        "# #select(),\n",
        "# # where(), groupBy(), and count()\n",
        "#  All transformations can be seen as pipes that ingest a structure and\n",
        "#  return a modified structure.\n"
      ],
      "metadata": {
        "id": "GbGfZVtLF7Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# We can avoid intermediate variables by chaining the results of one method to the next.\n",
        "#  Since each transformation returns a data frame (or GroupedData, when we perform\n",
        "#  the groupby() method), we can directly append the next method without assigning\n",
        "#  the result to a variable. This means that we can eschew all but one variable assign\n",
        "# ment."
      ],
      "metadata": {
        "id": "2bbPw8zWGgCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-biBydbHFmS",
        "outputId": "f88677e4-ec71-469a-f4dc-0bacf51a28ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/cde_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJaiKMUG6l6",
        "outputId": "7895b3c5-90d2-4e18-ffe2-d3b763ad8a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1342-0.txt\n",
            " data.csv\n",
            " prideandprejudice.csv\n",
            "'pyngrok_UI_CODE_Working_with_RDDs_in_PySpark_(2).ipynb'\n",
            " pyspark_tutorial.ipynb\n",
            " sales_data.csv\n",
            " sample.csv\n",
            " sample_data.csv\n",
            " Section_2_Resilient_Distributed_Datasets_Transformations.ipynb\n",
            " Section_3_Resilient_Distributed_Datasets_Actions.ipynb\n",
            " Section_4_Spark_DataFrames_and_Transformations.ipynb\n",
            " simple_count.csv\n",
            " simple_count_single_partition.csv\n",
            " simple_count_single_partition_final.csv\n",
            " Spark_SQL.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        " # Before\n",
        "book = spark.read.text(\"/content/drive/MyDrive/cde_data/1342-0.txt\")\n",
        "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
        "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
        "words_lower = words.select(lower(col(\"word\")).alias(\"word\"))\n",
        "words_clean = words_lower.select(\n",
        "regexp_extract(col(\"word\"), \"[a-z']*\", 0).alias(\"word\")\n",
        " )\n",
        "words_nonull = words_clean.where(col(\"word\") != \"\")\n",
        "results = words_nonull.groupby(\"word\").count()\n",
        "results.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZULQqURVGhnv",
        "outputId": "3f840f3d-fc6e-4c0a-b462-81ea0b53cef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|         word|count|\n",
            "+-------------+-----+\n",
            "|       online|    4|\n",
            "|         some|  203|\n",
            "|        still|   72|\n",
            "|          few|   72|\n",
            "|         hope|  122|\n",
            "|        those|   60|\n",
            "|     cautious|    4|\n",
            "|       lady's|    8|\n",
            "|    imitation|    1|\n",
            "|          art|    3|\n",
            "|      solaced|    1|\n",
            "|       poetry|    2|\n",
            "|    arguments|    5|\n",
            "| premeditated|    1|\n",
            "|      elevate|    1|\n",
            "|       doubts|    2|\n",
            "|    destitute|    1|\n",
            "|    solemnity|    5|\n",
            "|gratification|    1|\n",
            "|    connected|   14|\n",
            "+-------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# from pyspark.sql.functions import (\n",
        "#     col,\n",
        "#     explode,\n",
        "#     lower,\n",
        "#     regexp_extract,\n",
        "#     split,\n",
        "#  )\n",
        "# spark = SparkSession.builder.appName(\n",
        "#     \"Analyzing the vocabulary of Pride and Prejudice.\"\n",
        "#  ).getOrCreate()\n",
        "book_final = spark.read.text(\"/content/drive/MyDrive/cde_data/1342-0.txt\")\n",
        "\n",
        " # After chaining\n",
        "import pyspark.sql.functions as F\n",
        "results1 = (\n",
        "    spark.read.text(\"/content/drive/MyDrive/cde_data/1342-0.txt\")\n",
        "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
        "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
        "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
        "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
        "    .where(F.col(\"word\") != \"\")\n",
        "    .groupby(\"word\")\n",
        "    .count()\n",
        " )\n",
        "results1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6ZN3NgZHV3a",
        "outputId": "63c4028c-7621-4a54-cd04-2da7dd008e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|         word|count|\n",
            "+-------------+-----+\n",
            "|       online|    4|\n",
            "|         some|  203|\n",
            "|        still|   72|\n",
            "|          few|   72|\n",
            "|         hope|  122|\n",
            "|        those|   60|\n",
            "|     cautious|    4|\n",
            "|       lady's|    8|\n",
            "|    imitation|    1|\n",
            "|          art|    3|\n",
            "|      solaced|    1|\n",
            "|       poetry|    2|\n",
            "|    arguments|    5|\n",
            "| premeditated|    1|\n",
            "|      elevate|    1|\n",
            "|       doubts|    2|\n",
            "|    destitute|    1|\n",
            "|    solemnity|    5|\n",
            "|gratification|    1|\n",
            "|    connected|   14|\n",
            "+-------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gutenberg_books"
      ],
      "metadata": {
        "id": "yg1EnwT9Hw1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # After\n",
        "results_all = spark.read.text('/content/drive/MyDrive/cde_data/*.txt')\n",
        "results_all.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeUQyRzDJURw",
        "outputId": "e71509e3-b68e-4fe5-92f4-46f99e7dfdef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|The Project Guten...|\n",
            "|                    |\n",
            "|This eBook is for...|\n",
            "|almost no restric...|\n",
            "|re-use it under t...|\n",
            "|with this eBook o...|\n",
            "|                    |\n",
            "|                    |\n",
            "|Title: Pride and ...|\n",
            "|                    |\n",
            "| Author: Jane Austen|\n",
            "|                    |\n",
            "|Posting Date: Aug...|\n",
            "|Release Date: Jun...|\n",
            "|Last Updated: Mar...|\n",
            "|                    |\n",
            "|   Language: English|\n",
            "|                    |\n",
            "|Character set enc...|\n",
            "|                    |\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IiAO10C1Jowk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}